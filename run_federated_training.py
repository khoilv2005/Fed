################################################################################
#                                                                              #
#  SCRIPT HUáº¤N LUYá»†N FEDERATED LEARNING (FLOWER) HOÃ€N CHá»ˆNH                   #
#  Framework: Tá»± build (PyTorch) - 1 FILE DUY NHáº¤T                             #
#  MÃ´ hÃ¬nh: CNN-GRU Äáº¦Y Äá»¦ (Full Model)                                        #
#  Chiáº¿n lÆ°á»£c: ğŸŒŸ FedAvg & FedProx (Tá»± code) ğŸŒŸ                                #
#  TÃ­nh nÄƒng: âš¡ Tá»I Æ¯U GPU + SONG SONG (Multiprocessing) + BÃO CÃO F1-Score   #
#                                                                              #
################################################################################

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, Subset, TensorDataset
import torch.multiprocessing as mp  # ThÃªm thÆ° viá»‡n multiprocessing
import numpy as np
import matplotlib.pyplot as plt
import os
import logging
import pickle
import json
import copy
from collections import OrderedDict
from typing import List, Dict, Tuple
from datetime import datetime
from tqdm.auto import tqdm  # âœ… ThÃªm tqdm cho progress bar
import time  # âœ… Äá»ƒ Ä‘o thá»i gian má»—i batch


# HÃ m check gpu
def check_and_setup_gpu(config: Dict) -> str:
    """
    Kiá»ƒm tra xem GPU (CUDA) cÃ³ kháº£ dá»¥ng hay khÃ´ng,
    vÃ  thiáº¿t láº­p thiáº¿t bá»‹ ('cuda' hoáº·c 'cpu') Ä‘á»ƒ sá»­ dá»¥ng.
    """

    # Kiá»ƒm tra cáº¥u hÃ¬nh vÃ  kháº£ nÄƒng cá»§a há»‡ thá»‘ng
    if config.get('force_gpu', False) and not torch.cuda.is_available():
        device = 'cpu'
        logger.warning(
            "âš ï¸ Lá»–I Cáº¤U HÃŒNH: Báº¡n yÃªu cáº§u 'force_gpu=True' nhÆ°ng khÃ´ng tÃ¬m tháº¥y GPU (CUDA). "
            "Buá»™c pháº£i chuyá»ƒn sang cháº¡y trÃªn CPU."
        )
    elif torch.cuda.is_available() and config['device'] == 'cuda':
        device = 'cuda'
        # Thiáº¿t láº­p device Ä‘á»ƒ in ra thÃ´ng tin GPU
        current_device = torch.cuda.current_device()
        device_name = torch.cuda.get_device_name(current_device)
        logger.info(f"âœ… ÄÃ£ phÃ¡t hiá»‡n vÃ  sá»­ dá»¥ng GPU/CUDA: {device_name}")
    else:
        # Náº¿u cáº¥u hÃ¬nh lÃ  'cpu' hoáº·c khÃ´ng cÃ³ GPU
        device = 'cpu'
        logger.info("âš™ï¸ Cháº¡y trÃªn CPU theo cáº¥u hÃ¬nh hoáº·c do khÃ´ng tÃ¬m tháº¥y GPU.")

    # Cáº­p nháº­t cáº¥u hÃ¬nh vÃ  tráº£ vá» thiáº¿t bá»‹
    config['device'] = device
    return device


# ============================================
# === THÃŠM THÆ¯ VIá»†N CHO BIá»‚U Äá»’ VÃ€ Káº¾T QUáº¢ ===
import seaborn as sns
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    precision_recall_fscore_support
)
import pandas as pd
# ============================================

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

# ============================================================================
# ğŸ’¡ BÆ¯á»šC 1: Cáº¤U HÃŒNH CHÃNH ğŸ’¡
# ============================================================================

CONFIG = {
    # â¬‡ï¸ CHá»ˆNH ÄÆ¯á»œNG DáºªN NÃ€Y (TRÃŠN COLAB Cáº¦N MOUNT DRIVE TRÆ¯á»šC) â¬‡ï¸
    'data_dir': '/content/drive/MyDrive/Fed-Data/5-Client',
    'output_dir': '/content/drive/MyDrive/Fed-Data/5-Client/Results',  # LÆ°u káº¿t quáº£

    'num_clients': 5,

    # Model params (sáº½ Ä‘Æ°á»£c tá»± Ä‘á»™ng phÃ¡t hiá»‡n tá»« data)
    'input_shape': None,  # Tá»± Ä‘á»™ng phÃ¡t hiá»‡n
    'num_classes': None,  # Tá»± Ä‘á»™ng phÃ¡t hiá»‡n

    # Training params
    'algorithm': 'fedavg',     # 'fedavg' hoáº·c 'fedprox'
    'num_rounds': 10,           # Giáº£m sá»‘ round cho cháº¡y thá»­ nghiá»‡m nhanh
    'local_epochs': 5,         # 1 Epoch/VÃ²ng
    'learning_rate': 0.001,
    'batch_size': 1024,        # Batch size lá»›n (GPU 15GB ok)
    'client_fraction': 1.0,    # Tá»‰ lá»‡ clients tham gia má»—i round

    # FedProx specific
    'mu': 0.01,  # Proximal term coefficient

    # Device - LuÃ´n Æ°u tiÃªn GPU
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'force_gpu': True,  # Set False náº¿u muá»‘n cho phÃ©p cháº¡y trÃªn CPU

    # Multiprocessing
    'use_multiprocessing': False,  # Cháº¡y clients song song
    'num_processes': 1,           # Giáº£m sá»‘ processes cho cháº¡y thá»­ nghiá»‡m nhanh

    # Visualization
    'eval_every': 1,
}

# === Táº O THÆ¯ Má»¤C OUTPUT ===
TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")
OUTPUT_DIR = os.path.join(CONFIG['output_dir'], f"run_{TIMESTAMP}_{CONFIG['algorithm']}")
os.makedirs(OUTPUT_DIR, exist_ok=True)
CONFIG['output_dir'] = OUTPUT_DIR  # Cáº­p nháº­t config vá»›i Ä‘Æ°á»ng dáº«n má»›i

# ============================================================================
# ğŸ’¡ BÆ¯á»šC 2: Äá»ŠNH NGHÄ¨A MÃ” HÃŒNH CNN-GRU ğŸ’¡
# ============================================================================


class CNN_GRU_Model(nn.Module):
    """
    MÃ´ hÃ¬nh CNN-GRU (CNN + GRU + MLP + Softmax) báº±ng PyTorch
    (PhiÃªn báº£n Tá»I Æ¯U Tá»C Äá»˜, Ä‘Ã£ táº¯t recurrent_dropout)
    """
    def __init__(self, input_shape, num_classes=2):
        super(CNN_GRU_Model, self).__init__()

        if isinstance(input_shape, tuple):
            seq_length = input_shape[0]
        else:
            seq_length = input_shape

        self.input_shape = input_shape
        self.num_classes = num_classes

        # ===== CNN MODULE =====
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm1d(64)
        self.pool1 = nn.MaxPool1d(kernel_size=2)
        self.dropout_cnn1 = nn.Dropout(0.2)

        # Conv Block 2
        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm1d(128)
        self.pool2 = nn.MaxPool1d(kernel_size=2)
        self.dropout_cnn2 = nn.Dropout(0.2)

        # Conv Block 3
        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm1d(256)
        self.pool3 = nn.MaxPool1d(kernel_size=2)
        self.dropout_cnn3 = nn.Dropout(0.3)

        # TÃ­nh toÃ¡n kÃ­ch thÆ°á»›c output cá»§a CNN
        def conv_output_shape(L_in, kernel_size=1, stride=1, padding=0, dilation=1):
            return (L_in + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1

        cnn_output_length = seq_length
        cnn_output_length = conv_output_shape(cnn_output_length, kernel_size=3, stride=1, padding=1)  # conv1
        cnn_output_length = conv_output_shape(cnn_output_length, kernel_size=2, stride=2)            # pool1
        cnn_output_length = conv_output_shape(cnn_output_length, kernel_size=3, stride=1, padding=1)  # conv2
        cnn_output_length = conv_output_shape(cnn_output_length, kernel_size=2, stride=2)            # pool2
        cnn_output_length = conv_output_shape(cnn_output_length, kernel_size=3, stride=1, padding=1)  # conv3
        cnn_output_length = conv_output_shape(cnn_output_length, kernel_size=2, stride=2)            # pool3

        self.cnn_output_size = 256 * cnn_output_length

        # ===== GRU MODULE =====
        self.gru1 = nn.GRU(input_size=1, hidden_size=128, batch_first=True)
        self.gru2 = nn.GRU(input_size=128, hidden_size=64, batch_first=True)
        self.gru_output_size = 64

        # ===== MLP MODULE =====
        concat_size = self.cnn_output_size + self.gru_output_size

        self.dense1 = nn.Linear(concat_size, 256)
        self.bn_mlp1 = nn.BatchNorm1d(256)
        self.dropout1 = nn.Dropout(0.4)

        self.dense2 = nn.Linear(256, 128)
        self.bn_mlp2 = nn.BatchNorm1d(128)
        self.dropout2 = nn.Dropout(0.3)

        self.output = nn.Linear(128, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        if len(x.shape) == 2:
            x = x.unsqueeze(-1)

        batch_size = x.size(0)

        # ===== CNN =====
        x_cnn = x.permute(0, 2, 1)

        x_cnn = self.pool1(self.relu(self.bn1(self.conv1(x_cnn))))
        x_cnn = self.dropout_cnn1(x_cnn)

        x_cnn = self.pool2(self.relu(self.bn2(self.conv2(x_cnn))))
        x_cnn = self.dropout_cnn2(x_cnn)

        x_cnn = self.pool3(self.relu(self.bn3(self.conv3(x_cnn))))
        x_cnn = self.dropout_cnn3(x_cnn)

        cnn_output = x_cnn.view(batch_size, -1)

        # ===== GRU =====
        x_gru = x
        x_gru, _ = self.gru1(x_gru)
        x_gru, _ = self.gru2(x_gru)
        gru_output = x_gru[:, -1, :]

        # ===== CONCAT =====
        concatenated = torch.cat([cnn_output, gru_output], dim=1)

        # ===== MLP =====
        x = self.dense1(concatenated)
        if x.shape[0] > 1:  # BatchNorm yÃªu cáº§u batch_size > 1
            x = self.bn_mlp1(x)
        x = self.relu(x)
        x = self.dropout1(x)

        x = self.dense2(x)
        if x.shape[0] > 1:
            x = self.bn_mlp2(x)
        x = self.relu(x)
        x = self.dropout2(x)

        out = self.output(x)
        return out


def build_cnn_gru_model(input_shape, num_classes=2):
    """HÃ m tiá»‡n Ã­ch Ä‘á»ƒ khá»Ÿi táº¡o model CNN-GRU."""
    model = CNN_GRU_Model(input_shape, num_classes)
    print(f"\nâœ… Khá»Ÿi táº¡o mÃ´ hÃ¬nh CNN-GRU thÃ nh cÃ´ng")
    print(f"   - KÃ­ch thÆ°á»›c input: {input_shape}")
    print(f"   - Sá»‘ lá»›p (num_classes): {num_classes}")
    return model


# ============================================================================
# ğŸ’¡ BÆ¯á»šC 3: Äá»ŠNH NGHÄ¨A CLIENT FEDERATED ğŸ’¡
# ============================================================================

class FederatedClient:
    """
    Má»—i client cÃ³ dá»¯ liá»‡u riÃªng vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh local.
    """
    def __init__(
        self,
        client_id: int,
        model: nn.Module,
        train_loader: DataLoader,
        test_loader: DataLoader = None,
        device: str = 'cpu'
    ):
        self.client_id = client_id
        self.model = model
        self.train_loader = train_loader
        self.test_loader = test_loader
        self.device = device
        self.model.to(device)

    def get_model_params(self) -> OrderedDict:
        """Láº¥y tham sá»‘ mÃ´ hÃ¬nh cá»§a client."""
        return copy.deepcopy(self.model.state_dict())

    def set_model_params(self, params: OrderedDict):
        """Cáº­p nháº­t tham sá»‘ cho mÃ´ hÃ¬nh client."""
        self.model.load_state_dict(params)

    def train_fedavg(
        self,
        epochs: int,
        learning_rate: float = 0.01,
        verbose: int = 1
    ) -> Dict:
        """
        Huáº¥n luyá»‡n local vá»›i FedAvg.
        CÃ³ progress bar chi tiáº¿t cho tá»«ng batch.
        """
        self.model.train()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()

        total_loss = 0.0
        total_samples = 0

        for epoch in range(epochs):
            epoch_loss = 0.0
            epoch_samples = 0

            if verbose:
                pbar = tqdm(
                    self.train_loader,
                    desc=f"[Client {self.client_id}] FedAvg Epoch {epoch+1}/{epochs}",
                    unit="batch",
                    leave=False
                )
            else:
                pbar = self.train_loader

            for batch_idx, (data, target) in enumerate(pbar):
                batch_start = time.time()

                data, target = data.to(self.device), target.to(self.device)

                optimizer.zero_grad()
                output = self.model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()

                batch_time = time.time() - batch_start

                epoch_loss += loss.item() * data.size(0)
                epoch_samples += data.size(0)

                if verbose:
                    pbar.set_postfix({
                        "loss": f"{loss.item():.4f}",
                        "lr": f"{optimizer.param_groups[0]['lr']:.1e}",
                        "bt": f"{batch_time*1000:.0f}ms"
                    })

            avg_loss = epoch_loss / max(1, epoch_samples)
            total_loss += epoch_loss
            total_samples += epoch_samples

            if verbose:
                print(f"\nClient {self.client_id} - Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}")

        avg_total_loss = total_loss / max(1, total_samples)

        return {
            'client_id': self.client_id,
            'num_samples': total_samples // max(1, epochs),
            'loss': avg_total_loss
        }

    def train_fedprox(
        self,
        epochs: int,
        global_params: OrderedDict,
        mu: float = 0.01,
        learning_rate: float = 0.01,
        verbose: int = 0
    ) -> Dict:
        """
        Train model vá»›i FedProx:
        loss = CE + (mu/2) * ||w - w_global||^2
        CÃ³ progress bar hiá»ƒn thá»‹ CE loss + Prox term.
        """
        self.model.train()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()

        total_loss = 0.0
        total_samples = 0

        for epoch in range(epochs):
            epoch_loss = 0.0
            epoch_samples = 0

            if verbose:
                pbar = tqdm(
                    self.train_loader,
                    desc=f"[Client {self.client_id}] FedProx Epoch {epoch+1}/{epochs}",
                    unit="batch",
                    leave=False
                )
            else:
                pbar = self.train_loader

            for batch_idx, (data, target) in enumerate(pbar):
                batch_start = time.time()

                data, target = data.to(self.device), target.to(self.device)

                optimizer.zero_grad()
                output = self.model(data)

                # Loss chuáº©n (cross entropy)
                ce_loss = criterion(output, target)

                # Proximal Term
                proximal_term = 0.0
                for name, param in self.model.named_parameters():
                    if param.requires_grad:
                        global_param = global_params[name].to(self.device)
                        proximal_term += torch.sum((param - global_param) ** 2)

                proximal_term = (mu / 2) * proximal_term
                loss = ce_loss + proximal_term

                loss.backward()
                optimizer.step()

                batch_time = time.time() - batch_start

                epoch_loss += ce_loss.item() * data.size(0)  # chá»‰ log CE
                epoch_samples += data.size(0)

                if verbose:
                    prox_val = float(proximal_term.detach().cpu().item())
                    pbar.set_postfix({
                        "ce": f"{ce_loss.item():.4f}",
                        "prox": f"{prox_val:.2e}",
                        "lr": f"{optimizer.param_groups[0]['lr']:.1e}",
                        "bt": f"{batch_time*1000:.0f}ms"
                    })

            avg_loss = epoch_loss / max(1, epoch_samples)
            total_loss += epoch_loss
            total_samples += epoch_samples

            if verbose:
                print(f"\nClient {self.client_id} - Epoch {epoch+1}/{epochs}, Avg CE Loss: {avg_loss:.4f}")

        avg_total_loss = total_loss / max(1, total_samples)

        return {
            'client_id': self.client_id,
            'num_samples': total_samples // max(1, epochs),
            'loss': avg_total_loss
        }

    def evaluate(self) -> Dict:
        """ÄÃ¡nh giÃ¡ model trÃªn test set cá»§a client."""
        if self.test_loader is None:
            return {'accuracy': 0.0, 'loss': 0.0, 'num_samples': 0}

        self.model.eval()
        criterion = nn.CrossEntropyLoss()
        total_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                loss = criterion(output, target)

                total_loss += loss.item() * data.size(0)
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += data.size(0)

        accuracy = correct / total if total > 0 else 0.0
        avg_loss = total_loss / total if total > 0 else 0.0

        return {
            'accuracy': accuracy,
            'loss': avg_loss,
            'num_samples': total
        }


# ============================================================================
# ğŸ’¡ BÆ¯á»šC 4: Äá»ŠNH NGHÄ¨A SERVER FEDERATED ğŸ’¡
# ============================================================================

class FederatedServer:
    """
    Server quáº£n lÃ½ global model vÃ  thá»±c hiá»‡n aggregation.
    """
    def __init__(
        self,
        model: nn.Module,
        clients: List[FederatedClient],
        client_test_loaders: List[DataLoader],
        device: str = 'cpu'
    ):
        self.global_model = model
        self.clients = clients
        self.client_test_loaders = client_test_loaders
        self.device = device
        self.global_model.to(device)

        self.history = {
            'train_loss': [],
            'test_accuracy': [],
            'test_loss': []
        }

    def get_global_params(self) -> OrderedDict:
        return copy.deepcopy(self.global_model.state_dict())

    def set_global_params(self, params: OrderedDict):
        self.global_model.load_state_dict(params)

    def distribute_model(self, client_list: List[FederatedClient]):
        """Gá»­i tham sá»‘ mÃ´ hÃ¬nh toÃ n cá»¥c xuá»‘ng cÃ¡c client Ä‘Æ°á»£c chá»n."""
        global_params = self.get_global_params()
        for client in client_list:
            client.set_model_params(global_params)

    def aggregate_fedavg(self, client_results: List[Dict]) -> OrderedDict:
        """
        FedAvg aggregation (Fix dtype & BatchNorm).
        """
        total_samples = sum(result['num_samples'] for result in client_results)

        aggregated_params = self.get_global_params()

        # Äáº·t táº¥t cáº£ cÃ¡c tham sá»‘ float vá» 0
        for key in aggregated_params.keys():
            if aggregated_params[key].dtype in [torch.float32, torch.float64, torch.float16]:
                aggregated_params[key] = torch.zeros_like(aggregated_params[key])

        # Weighted sum (chá»‰ cho cÃ¡c tham sá»‘ float)
        for result in client_results:
            client_id = result['client_id']
            num_samples = result['num_samples']
            weight = num_samples / max(1, total_samples)

            client = self.clients[client_id]
            client_params = client.get_model_params()

            for key in aggregated_params.keys():
                param = client_params[key]
                if param.dtype in [torch.float32, torch.float64, torch.float16]:
                    weight_tensor = torch.tensor(weight, dtype=param.dtype, device=param.device)
                    if aggregated_params[key].device != param.device:
                        aggregated_params[key] = aggregated_params[key].to(param.device)

                    aggregated_params[key] += weight_tensor * param
                else:
                    # Giá»¯ láº¡i giÃ¡ trá»‹ cá»§a client Ä‘áº§u tiÃªn
                    if client_id == client_results[0]['client_id']:
                        aggregated_params[key] = param

        return aggregated_params

    def train_round_fedavg(
        self,
        num_epochs: int,
        learning_rate: float = 0.01,
        client_fraction: float = 1.0,
        verbose: int = 1
    ) -> Dict:
        """
        Thá»±c hiá»‡n 1 round huáº¥n luyá»‡n vá»›i FedAvg. (Tuáº§n tá»±)
        """
        num_selected = max(1, int(len(self.clients) * client_fraction))
        selected_clients = np.random.choice(self.clients, num_selected, replace=False)

        if verbose:
            print(f"â†’ [Round] Chá»n {len(selected_clients)} client Ä‘á»ƒ huáº¥n luyá»‡n...")

        self.distribute_model(selected_clients)

        client_results = []
        for idx, client in enumerate(selected_clients):
            if verbose:
                num_batches = len(client.train_loader)
                print(f"\nâ†’ Training Client {client.client_id} ({idx+1}/{num_selected}) - "
                      f"{len(client.train_loader.dataset):,} samples, {num_batches} batches")

            result = client.train_fedavg(
                epochs=num_epochs,
                learning_rate=learning_rate,
                verbose=verbose
            )
            client_results.append(result)
            if verbose:
                print(f"   âœ“ Client {client.client_id} completed - Avg Loss: {result['loss']:.4f}")

        if verbose:
            print(f"\nâ†’ [Round] Äang tá»•ng há»£p (aggregating) {len(client_results)} mÃ´ hÃ¬nh...")

        aggregated_params = self.aggregate_fedavg(client_results)
        self.set_global_params(aggregated_params)

        avg_loss = float(np.mean([r['loss'] for r in client_results])) if client_results else 0.0

        if verbose:
            print(f"â†’ [Round] HoÃ n thÃ nh, Loss trung bÃ¬nh (train): {avg_loss:.4f}")

        return {'train_loss': avg_loss, 'num_clients': len(selected_clients)}

    def train_round_fedprox(
        self,
        num_epochs: int,
        mu: float = 0.01,
        learning_rate: float = 0.01,
        client_fraction: float = 1.0,
        verbose: int = 0
    ) -> Dict:
        """
        Thá»±c hiá»‡n 1 round huáº¥n luyá»‡n vá»›i FedProx. (Tuáº§n tá»±)
        """
        num_selected = max(1, int(len(self.clients) * client_fraction))
        selected_clients = np.random.choice(self.clients, num_selected, replace=False)

        if verbose:
            print(f"â†’ [Round] Chá»n {len(selected_clients)} client Ä‘á»ƒ huáº¥n luyá»‡n (FedProx)...")

        global_params = self.get_global_params()
        self.distribute_model(selected_clients)

        client_results = []
        for idx, client in enumerate(selected_clients):
            if verbose:
                num_batches = len(client.train_loader)
                print(f"\nâ†’ Training Client {client.client_id} ({idx+1}/{num_selected}) - "
                      f"{len(client.train_loader.dataset):,} samples, {num_batches} batches")

            result = client.train_fedprox(
                epochs=num_epochs,
                global_params=global_params,
                mu=mu,
                learning_rate=learning_rate,
                verbose=verbose
            )
            client_results.append(result)
            if verbose:
                print(f"   âœ“ Client {client.client_id} completed - Avg CE Loss: {result['loss']:.4f}")

        if verbose:
            print(f"\nâ†’ [Round] Äang tá»•ng há»£p (aggregating) {len(client_results)} mÃ´ hÃ¬nh...")

        aggregated_params = self.aggregate_fedavg(client_results)
        self.set_global_params(aggregated_params)

        avg_loss = float(np.mean([r['loss'] for r in client_results])) if client_results else 0.0

        if verbose:
            print(f"â†’ [Round] HoÃ n thÃ nh, Loss CE trung bÃ¬nh (train): {avg_loss:.4f}")

        return {'train_loss': avg_loss, 'num_clients': len(selected_clients)}

    def evaluate_global(self) -> Dict:
        """
        ÄÃ¡nh giÃ¡ global model trÃªn Táº¤T Cáº¢ test set cá»§a client (láº·p qua tá»«ng client),
        cÃ³ progress bar chi tiáº¿t.
        """
        if self.client_test_loaders is None:
            print("âš ï¸  [Server Evaluate] KhÃ´ng tÃ¬m tháº¥y client_test_loaders.")
            return {'accuracy': 0.0, 'loss': 0.0}

        self.global_model.eval()
        criterion = nn.CrossEntropyLoss()

        total_loss = 0.0
        correct = 0
        total = 0

        for loader_idx, loader in enumerate(self.client_test_loaders):
            pbar = tqdm(
                loader,
                desc=f"[Eval] Client TestLoader {loader_idx}",
                unit="batch",
                leave=False
            )
            with torch.no_grad():
                for data, target in pbar:
                    data, target = data.to(self.device), target.to(self.device)
                    output = self.global_model(data)
                    loss = criterion(output, target)

                    total_loss += loss.item() * data.size(0)
                    pred = output.argmax(dim=1)
                    correct += pred.eq(target).sum().item()
                    total += data.size(0)

                    if total > 0:
                        current_acc = correct / total
                        current_loss = total_loss / total
                        pbar.set_postfix({
                            "acc": f"{current_acc*100:.2f}%",
                            "loss": f"{current_loss:.4f}"
                        })

        accuracy = correct / total if total > 0 else 0.0
        avg_loss = total_loss / total if total > 0 else 0.0

        return {
            'accuracy': accuracy,
            'loss': avg_loss
        }


# ============================================================================
# ğŸ’¡ BÆ¯á»šC 5: HÃ€M Tá»° Äá»˜NG PHÃT HIá»†N THAM Sá» Dá»® LIá»†U ğŸ’¡
# ============================================================================

def auto_detect_data_parameters(data_dir, num_clients):
    """
    Tá»± Ä‘á»™ng phÃ¡t hiá»‡n input_shape vÃ  num_classes.
    """
    print("\n" + "="*80)
    print("ğŸ“‚ Tá»° Äá»˜NG PHÃT HIá»†N THAM Sá» Dá»® LIá»†U")
    print("="*80)
    print(f"â†’ ThÆ° má»¥c dá»¯ liá»‡u: {data_dir}")
    print(f"â†’ Sá»‘ lÆ°á»£ng client (dá»± kiáº¿n): {num_clients}")

    try:
        all_labels = []
        data_stats = {}

        # Láº¥y kÃ­ch thÆ°á»›c input tá»« client_0
        client_0_path = os.path.join(data_dir, "client_0_data.npz")
        if not os.path.exists(client_0_path):
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file: {client_0_path}")

        with np.load(client_0_path) as data:
            x_train_sample = data['X_train']
            input_features = x_train_sample.shape[1]
            input_shape = (input_features,)
            print(f"\nâœ… ThÃ´ng tin tá»« client 0:")
            print(f"   - Sá»‘ Ä‘áº·c trÆ°ng (INPUT_FEATURES): {input_features}")
            print(f"   - input_shape: {input_shape}")

        print(f"\nâ†’ Äang quÃ©t dá»¯ liá»‡u cá»§a {num_clients} client Ä‘á»ƒ thá»‘ng kÃª nhÃ£n...")
        total_train = 0
        total_test = 0

        for i in range(num_clients):
            file_path = os.path.join(data_dir, f"client_{i}_data.npz")
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file: {file_path}")

            with np.load(file_path) as data:
                x_train = data['X_train']; y_train = data['y_train']
                x_test = data['X_test']; y_test = data['y_test']

                all_labels.append(y_train)
                unique_labels, counts = np.unique(y_train, return_counts=True)
                total_train += len(x_train)
                total_test += len(x_test)

                data_stats[i] = {
                    'train_samples': int(len(x_train)),
                    'test_samples': int(len(x_test)),
                    'unique_labels': int(len(unique_labels)),
                    'label_distribution': {str(k): int(v) for k, v in zip(unique_labels, counts)}
                }
                print(f"   - Client {i}: {len(x_train)} máº«u train, {len(x_test)} máº«u test, {len(unique_labels)} nhÃ£n")

        combined_labels = np.concatenate(all_labels)
        num_classes = len(np.unique(combined_labels))

        print("\nğŸ“Š Tá»•ng há»£p toÃ n bá»™ dá»¯ liá»‡u:")
        print(f"   - Sá»‘ lá»›p (num_classes): {num_classes}")
        print(f"   - Tá»•ng sá»‘ máº«u train: {total_train:,}")
        print(f"   - Tá»•ng sá»‘ máº«u test:  {total_test:,}")
        print("="*80)

        return input_shape, num_classes, data_stats

    except FileNotFoundError as e:
        print("\n" + "="*80 +
              f"\nâŒ Lá»–I: KHÃ”NG TÃŒM THáº¤Y Tá»†P Dá»® LIá»†U\nÄÆ°á»ng dáº«n: {e.filename}\n" + "="*80)
        raise
    except KeyError as e:
        print("\n" + "="*80 +
              f"\nâŒ Lá»–I: THIáº¾U KEY TRONG FILE .NPZ\nKey: {e}\n" + "="*80)
        raise


# ============================================================================
# ğŸ’¡ BÆ¯á»šC 6: HÃ€M LOAD Dá»® LIá»†U ğŸ’¡
# ============================================================================

class NumpyDataset(TensorDataset):
    """Dataset tiá»‡n dá»¥ng Ä‘á»ƒ wrap numpy array thÃ nh TensorDataset."""
    def __init__(self, X, y, device='cpu'):
        if len(X.shape) == 3:
            X = X.squeeze(-1)  # (N, F, 1) -> (N, F)

        X = X.astype(np.float32)
        X_tensor = torch.from_numpy(X)
        y_tensor = torch.from_numpy(y).long()
        super().__init__(X_tensor, y_tensor)


def load_federated_data(data_dir, num_clients, batch_size, device='cpu'):
    """
    Load dá»¯ liá»‡u federated cho táº¥t cáº£ client.
    """
    print("\n" + "="*80)
    print("ğŸ“¥ LOADING FEDERATED DATA")
    print("="*80)
    print(f"â†’ Thiáº¿t bá»‹ hiá»‡n dÃ¹ng: {device}")
    print(f"â†’ Sá»‘ lÆ°á»£ng client: {num_clients}\n")

    train_loaders = []
    test_loaders = []

    for client_id in range(num_clients):
        data_path = os.path.join(data_dir, f'client_{client_id}_data.npz')
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u cá»§a client {client_id} táº¡i: {data_path}")

        data = np.load(data_path)
        X_train = data['X_train']; y_train = data['y_train']
        X_test = data['X_test']; y_test = data['y_test']

        print(f"   - Client {client_id}: X_train {X_train.shape}, X_test {X_test.shape}")

        train_dataset = NumpyDataset(X_train, y_train, device)
        test_dataset = NumpyDataset(X_test, y_test, device)

        train_loader = DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True, drop_last=False
        )
        test_loader = DataLoader(
            test_dataset, batch_size=batch_size, shuffle=False, drop_last=False
        )
        train_loaders.append(train_loader)
        test_loaders.append(test_loader)

    print(f"\nâœ… ÄÃ£ load dá»¯ liá»‡u cho {num_clients} client.")
    print("="*80)

    return train_loaders, test_loaders


# ============================================================================
# ğŸ’¡ BÆ¯á»šC 7: HÃ€M KHá»I Táº O Há»† THá»NG ğŸ’¡
# ============================================================================

def initialize_federated_system(
    train_loaders,
    test_loaders,
    input_shape,
    num_classes,
    device='cpu'
):
    """
    Khá»Ÿi táº¡o global model, clients, vÃ  server
    """
    print("\n" + "="*80)
    print("ğŸ—ï¸  INITIALIZING FEDERATED SYSTEM")
    print("="*80)

    num_clients = len(train_loaders)

    # Táº¡o global model
    print(f"\nâ†’ Khá»Ÿi táº¡o mÃ´ hÃ¬nh toÃ n cá»¥c (global model)...")
    print(f"   - Input shape: {input_shape}")
    print(f"   - Sá»‘ lá»›p: {num_classes}")
    print(f"   - Thiáº¿t bá»‹: {device}")

    global_model = build_cnn_gru_model(input_shape, num_classes)
    global_model = global_model.to(device)
    print(f"   - MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c chuyá»ƒn sang thiáº¿t bá»‹: {device}")

    if isinstance(device, torch.device):
        device_type = device.type
    else:
        device_type = str(device)

    if device_type == 'cuda':
        print(f"   - XÃ¡c nháº­n tham sá»‘ Ä‘áº§u tiÃªn cá»§a mÃ´ hÃ¬nh Ä‘ang á»Ÿ: {next(global_model.parameters()).device}")

    total_params = sum(p.numel() for p in global_model.parameters())
    trainable_params = sum(p.numel() for p in global_model.parameters() if p.requires_grad)
    print(f"   - Tá»•ng sá»‘ tham sá»‘: {total_params:,}")
    print(f"   - Sá»‘ tham sá»‘ trainable: {trainable_params:,}")

    # Táº¡o clients
    print(f"\nâ†’ Khá»Ÿi táº¡o {num_clients} client...")
    clients = []

    for client_id in range(num_clients):
        client_model = CNN_GRU_Model(input_shape, num_classes)
        client_model.load_state_dict(global_model.state_dict())

        client = FederatedClient(
            client_id=client_id,
            model=client_model,
            train_loader=train_loaders[client_id],
            test_loader=test_loaders[client_id],
            device=device
        )
        clients.append(client)
        print(f"   - Client {client_id}: khá»Ÿi táº¡o thÃ nh cÃ´ng trÃªn thiáº¿t bá»‹ {device}")

    print("\nâ†’ GÃ¡n danh sÃ¡ch test loader cho server (trÃ¡nh lá»—i RAM)...")

    # Táº¡o server
    print("\nâ†’ Khá»Ÿi táº¡o server...")
    server = FederatedServer(
        model=global_model,
        clients=clients,
        client_test_loaders=test_loaders,
        device=device
    )

    print("\nâœ… Há»‡ thá»‘ng Federated Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o hoÃ n chá»‰nh.")
    print("="*80)

    return server, clients


# ============================================================================
# ğŸ’¡ BÆ¯á»šC 8: CÃC HÃ€M Há»– TRá»¢ MULTIPROCESSING ğŸ’¡
# ============================================================================

def _client_training_worker(args_tuple):
    """
    HÃ m worker (helper) Ä‘á»ƒ cháº¡y trong má»™t process riÃªng biá»‡t.
    CÃ³ tqdm riÃªng cho tá»«ng worker.
    """
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    import numpy as np
    from collections import OrderedDict
    from tqdm.auto import tqdm as _tqdm
    import time as _time

    class CNN_GRU_Model_Worker(nn.Module):
        def __init__(self, input_shape, num_classes=2):
            super(CNN_GRU_Model_Worker, self).__init__()
            if isinstance(input_shape, tuple):
                seq_length = input_shape[0]
            else:
                seq_length = input_shape
            self.input_shape = input_shape
            self.num_classes = num_classes
            self.conv1 = nn.Conv1d(1, 64, 3, padding=1)
            self.bn1 = nn.BatchNorm1d(64)
            self.pool1 = nn.MaxPool1d(2)
            self.dropout_cnn1 = nn.Dropout(0.2)
            self.conv2 = nn.Conv1d(64, 128, 3, padding=1)
            self.bn2 = nn.BatchNorm1d(128)
            self.pool2 = nn.MaxPool1d(2)
            self.dropout_cnn2 = nn.Dropout(0.2)
            self.conv3 = nn.Conv1d(128, 256, 3, padding=1)
            self.bn3 = nn.BatchNorm1d(256)
            self.pool3 = nn.MaxPool1d(2)
            self.dropout_cnn3 = nn.Dropout(0.3)

            def conv_output_shape(L_in, kernel_size=1, stride=1, padding=0, dilation=1):
                return (L_in + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1

            cnn_output_length = seq_length
            for _ in range(3):
                cnn_output_length = conv_output_shape(cnn_output_length, kernel_size=2, stride=2)
            self.cnn_output_size = 256 * cnn_output_length
            self.gru1 = nn.GRU(1, 128, batch_first=True)
            self.gru2 = nn.GRU(128, 64, batch_first=True)
            self.gru_output_size = 64
            concat_size = self.cnn_output_size + self.gru_output_size
            self.dense1 = nn.Linear(concat_size, 256)
            self.bn_mlp1 = nn.BatchNorm1d(256)
            self.dropout1 = nn.Dropout(0.4)
            self.dense2 = nn.Linear(256, 128)
            self.bn_mlp2 = nn.BatchNorm1d(128)
            self.dropout2 = nn.Dropout(0.3)
            self.output = nn.Linear(128, num_classes)
            self.relu = nn.ReLU()

        def forward(self, x):
            if len(x.shape) == 2:
                x = x.unsqueeze(-1)
            batch_size = x.size(0)
            x_cnn = x.permute(0, 2, 1)
            x_cnn = self.dropout_cnn1(self.pool1(self.relu(self.bn1(self.conv1(x_cnn)))))
            x_cnn = self.dropout_cnn2(self.pool2(self.relu(self.bn2(self.conv2(x_cnn)))))
            x_cnn = self.dropout_cnn3(self.pool3(self.relu(self.bn3(self.conv3(x_cnn)))))
            cnn_output = x_cnn.view(batch_size, -1)
            x_gru = x
            x_gru, _ = self.gru1(x_gru)
            x_gru, _ = self.gru2(x_gru)
            gru_output = x_gru[:, -1, :]
            concatenated = torch.cat([cnn_output, gru_output], dim=1)
            x = self.dense1(concatenated)
            if x.shape[0] > 1:
                x = self.bn_mlp1(x)
            x = self.relu(x)
            x = self.dropout1(x)
            x = self.dense2(x)
            if x.shape[0] > 1:
                x = self.bn_mlp2(x)
            x = self.relu(x)
            x = self.dropout2(x)
            return self.output(x)

    class NumpyDataset_Worker(TensorDataset):
        def __init__(self, X, y):
            if len(X.shape) == 3:
                X = X.squeeze(-1)
            X = X.astype(np.float32)
            X_tensor = torch.from_numpy(X)
            y_tensor = torch.from_numpy(y).long()
            super().__init__(X_tensor, y_tensor)

    try:
        (client_id, model_state_dict, train_data, device_id, config) = args_tuple

        num_epochs = config['local_epochs']
        learning_rate = config['learning_rate']
        algorithm = config['algorithm']
        mu = config['mu']
        batch_size = config['batch_size']

        if device_id != 'cpu' and torch.cuda.is_available():
            device = torch.device(f'cuda:{device_id}')
        else:
            device = torch.device('cpu')

        X_train, y_train = train_data

        train_dataset = NumpyDataset_Worker(X_train, y_train)
        train_loader = DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True, drop_last=False
        )

        model = CNN_GRU_Model_Worker(config['input_shape'], config['num_classes'])
        model.load_state_dict(model_state_dict)
        model = model.to(device)

        model.train()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()

        total_loss = 0.0
        total_samples = 0

        for epoch in range(num_epochs):
            epoch_loss = 0.0
            epoch_samples = 0

            pbar = _tqdm(
                train_loader,
                desc=f"[Worker Client {client_id}] Epoch {epoch+1}/{num_epochs}",
                unit="batch",
                leave=False
            )

            for data, target in pbar:
                batch_start = _time.time()

                data, target = data.to(device), target.to(device)

                optimizer.zero_grad()
                output = model(data)
                ce_loss = criterion(output, target)

                if algorithm == 'fedprox' and model_state_dict is not None:
                    proximal_term = 0.0
                    for name, param in model.named_parameters():
                        if param.requires_grad:
                            global_param = model_state_dict[name].to(device)
                            proximal_term += torch.sum((param - global_param) ** 2)
                    loss = ce_loss + (mu / 2) * proximal_term
                    prox_val = float(((mu / 2) * proximal_term).detach().cpu().item())
                else:
                    loss = ce_loss
                    prox_val = 0.0

                loss.backward()
                optimizer.step()

                batch_time = _time.time() - batch_start

                epoch_loss += ce_loss.item() * data.size(0)
                epoch_samples += data.size(0)

                pbar.set_postfix({
                    "ce": f"{ce_loss.item():.4f}",
                    "prox": f"{prox_val:.2e}",
                    "lr": f"{optimizer.param_groups[0]['lr']:.1e}",
                    "bt": f"{batch_time*1000:.0f}ms"
                })

            total_loss += epoch_loss
            total_samples += epoch_samples

        avg_loss = total_loss / max(1, total_samples)

        return {
            'client_id': client_id,
            'model_state_dict': {k: v.cpu() for k, v in model.state_dict().items()},
            'num_samples': len(X_train),
            'loss': avg_loss
        }

    except Exception as e:
        print(f"âŒ Lá»—i trong worker client {client_id}: {e}")
        import traceback
        traceback.print_exc()
        return None


def aggregate_models_fedavg_parallel(client_results: List[Dict]) -> OrderedDict:
    """
    FedAvg aggregation tá»« client results (Cháº¡y trÃªn CPU)
    """
    total_samples = sum(r['num_samples'] for r in client_results)

    aggregated_params = OrderedDict()
    first_state = client_results[0]['model_state_dict']

    for key in first_state.keys():
        aggregated_params[key] = torch.zeros_like(first_state[key])

    for result in client_results:
        weight = result['num_samples'] / max(1, total_samples)
        state_dict = result['model_state_dict']

        for key in aggregated_params.keys():
            param = state_dict[key]

            if param.dtype in [torch.float32, torch.float64, torch.float16]:
                weight_tensor = torch.tensor(weight, dtype=param.dtype, device=param.device)
                aggregated_params[key] += weight_tensor * param
            else:
                if result['client_id'] == client_results[0]['client_id']:
                    aggregated_params[key] = param

    return aggregated_params


def train_round_multiprocessing(
    server,
    config,
    train_loaders,
    device='cuda'
):
    """
    Train 1 round vá»›i multiprocessing - cháº¡y nhiá»u client song song.
    CÃ³ tqdm cho danh sÃ¡ch clients.
    """
    global_state_dict = {k: v.cpu() for k, v in server.get_global_params().items()}

    client_data = []
    for client_id, train_loader in enumerate(train_loaders):
        X_list, y_list = [], []
        for X_batch, y_batch in train_loader:
            X_list.append(X_batch.cpu().numpy())
            y_list.append(y_batch.cpu().numpy())
        X_train = np.concatenate(X_list, axis=0)
        y_train = np.concatenate(y_list, axis=0)
        client_data.append((X_train, y_train))

    if device == 'cuda' and torch.cuda.is_available() and torch.cuda.device_count() > 1:
        num_gpus = torch.cuda.device_count()
        device_ids = [i % num_gpus for i in range(config['num_clients'])]
        print(f"   â€¢ PhÃ¢n bá»• {config['num_clients']} clients cho {num_gpus} GPUs.")
    else:
        device_ids = [0 if device == 'cuda' else 'cpu'] * config['num_clients']

    args_list = [
        (
            client_id,
            global_state_dict,
            client_data[client_id],
            device_ids[client_id],
            config
        )
        for client_id in range(config['num_clients'])
    ]

    print(f"   â€¢ Báº¯t Ä‘áº§u train {config['num_clients']} clients song song vá»›i {config['num_processes']} processes...")

    mp_context = mp.get_context('spawn')
    results = []
    with mp_context.Pool(processes=config['num_processes']) as pool:
        for res in tqdm(
            pool.imap_unordered(_client_training_worker, args_list),
            total=len(args_list),
            desc="Clients (multiprocessing)",
            unit="client"
        ):
            results.append(res)

    results = [r for r in results if r is not None]

    if len(results) == 0:
        raise RuntimeError("Táº¥t cáº£ clients Ä‘á»u tháº¥t báº¡i!")

    print(f"   â€¢ Äang aggregate models tá»« {len(results)} clients...")
    aggregated_params_cpu = aggregate_models_fedavg_parallel(results)

    aggregated_params_gpu = OrderedDict(
        (k, v.to(device)) for k, v in aggregated_params_cpu.items()
    )
    server.set_global_params(aggregated_params_gpu)

    avg_loss = np.mean([r['loss'] for r in results])

    return {
        'train_loss': avg_loss,
        'num_clients': len(results)
    }


# ============================================================================
# ğŸ’¡ BÆ¯á»šC 9: HÃ€M HUáº¤N LUYá»†N CHÃNH ğŸ’¡
# ============================================================================

def train_federated(server, config, train_loaders=None):
    """
    Äiá»u phá»‘i quÃ¡ trÃ¬nh huáº¥n luyá»‡n (chá»n tuáº§n tá»± hoáº·c song song)
    CÃ³ tqdm cho vÃ²ng láº·p rounds.
    """
    print("\n" + "="*80)
    print("ğŸš€ Báº®T Äáº¦U HUáº¤N LUYá»†N FEDERATED")
    print("="*80)

    algorithm = config['algorithm']
    num_rounds = config['num_rounds']
    local_epochs = config['local_epochs']
    learning_rate = config['learning_rate']
    client_fraction = config['client_fraction']
    eval_every = config['eval_every']
    device = config['device']
    use_multiprocessing = config.get('use_multiprocessing', False)

    history = server.history

    print(f"\nğŸ“‹ Cáº¥u hÃ¬nh huáº¥n luyá»‡n:")
    print(f"   - Thuáº­t toÃ¡n: {algorithm.upper()}")
    print(f"   - Sá»‘ round: {num_rounds}")
    print(f"   - Sá»‘ epoch local: {local_epochs}")
    print(f"   - Learning rate: {learning_rate}")
    print(f"   - Batch size: {config['batch_size']}")
    print(f"   - Tá»‰ lá»‡ client má»—i round: {client_fraction}")
    print(f"   - Thiáº¿t bá»‹: {device}")
    print(f"   - Cháº¡y song song (Multiprocessing): {use_multiprocessing}")
    if use_multiprocessing:
        print(f"   - Sá»‘ Processes: {config['num_processes']}")
    if algorithm == 'fedprox':
        print(f"   - Mu (proximal term): {config['mu']}")

    if eval_every > 0:
        print("\nğŸ“Š ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh toÃ n cá»¥c (chÆ°a huáº¥n luyá»‡n)...")
        eval_result = server.evaluate_global()
        history['train_loss'].append(None)
        history['test_accuracy'].append(eval_result['accuracy'])
        history['test_loss'].append(eval_result['loss'])
        print(f"âœ… Round 0 (Init): Test Acc: {eval_result['accuracy']*100:.2f}% | Test Loss: {eval_result['loss']:.4f}")

    round_iter = tqdm(
        range(num_rounds),
        desc="Global Rounds",
        unit="round"
    )

    for round_idx in round_iter:
        print(f"\n{'='*60}")
        print(f"ğŸ“ ROUND {round_idx+1}/{num_rounds}")
        print(f"{'='*60}")

        if use_multiprocessing:
            if train_loaders is None:
                raise ValueError("train_loaders lÃ  báº¯t buá»™c khi dÃ¹ng multiprocessing")

            worker_config = config.copy()
            worker_config['local_epochs'] = local_epochs
            worker_config['learning_rate'] = learning_rate
            worker_config['algorithm'] = algorithm
            worker_config['mu'] = config.get('mu', 0.01)

            round_result = train_round_multiprocessing(
                server=server,
                config=worker_config,
                train_loaders=train_loaders,
                device=device
            )

        else:
            print("ğŸ“ Cháº¿ Ä‘á»™: SEQUENTIAL - Clients cháº¡y láº§n lÆ°á»£t (cháº­m)...")
            if algorithm == 'fedavg':
                round_result = server.train_round_fedavg(
                    num_epochs=local_epochs,
                    learning_rate=learning_rate,
                    client_fraction=client_fraction,
                    verbose=1
                )
            elif algorithm == 'fedprox':
                round_result = server.train_round_fedprox(
                    num_epochs=local_epochs,
                    mu=config['mu'],
                    learning_rate=learning_rate,
                    client_fraction=client_fraction,
                    verbose=1
                )
            else:
                raise ValueError(f"Thuáº­t toÃ¡n khÃ´ng há»— trá»£: {algorithm}")

        if (round_idx + 1) % eval_every == 0:
            print(f"\nğŸ“Š Äang Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh toÃ n cá»¥c trÃªn test set...")
            eval_result = server.evaluate_global()

            history['train_loss'].append(round_result['train_loss'])
            history['test_accuracy'].append(eval_result['accuracy'])
            history['test_loss'].append(eval_result['loss'])

            print(f"\nâœ… Round {round_idx+1}/{num_rounds} Summary:")
            print(f"   â€¢ Train Loss (Avg): {round_result['train_loss']:.4f}")
            print(f"   â€¢ Test Accuracy: {eval_result['accuracy']*100:.2f}%")
            print(f"   â€¢ Test Loss: {eval_result['loss']:.4f}")

            round_iter.set_postfix({
                "algo": algorithm,
                "train_loss": f"{round_result['train_loss']:.4f}",
                "test_acc": f"{eval_result['accuracy']*100:.2f}%"
            })

    if history['test_accuracy']:
        print(f"\nâœ… Huáº¥n luyá»‡n {algorithm.upper()} hoÃ n táº¥t.")
        print(f"   â†’ Äá»™ chÃ­nh xÃ¡c cuá»‘i cÃ¹ng trÃªn test: {history['test_accuracy'][-1]*100:.2f}%")
    else:
        print("\nâš ï¸ KhÃ´ng cÃ³ káº¿t quáº£ test nÃ o Ä‘Æ°á»£c ghi nháº­n.")

    return history


# ============================================================================
# ğŸ’¡ BÆ¯á»šC 10: HÃ€M Váº¼ BIá»‚U Äá»’ & LÆ¯U Káº¾T QUáº¢ ğŸ’¡
# ============================================================================

def plot_training_history(history, save_path):
    """
    Váº½ biá»ƒu Ä‘á»“ train_loss, test_loss vÃ  test_accuracy theo round.
    """
    print("\n" + "="*80)
    print("ğŸ“Š ÄANG Váº¼ BIá»‚U Äá»’ Káº¾T QUáº¢ HUáº¤N LUYá»†N")
    print("="*80)

    try:
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))

        rounds = range(len(history['test_loss']))
        train_loss = history['train_loss']
        test_loss = history['test_loss']
        test_acc = history['test_accuracy']

        ax1 = axes[0]
        ax1.plot(rounds[1:], train_loss[1:], label='Train Loss (Trung bÃ¬nh Client)',
                 marker='o', linewidth=2)
        ax1.plot(rounds, test_loss, label='Test Loss (ToÃ n cá»¥c)',
                 marker='s', linewidth=2)
        ax1.set_xlabel('Round', fontweight='bold')
        ax1.set_ylabel('Loss', fontweight='bold')
        ax1.set_title('Train Loss & Test Loss', fontweight='bold', fontsize=14)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_xticks(rounds)

        ax2 = axes[1]
        test_acc_pct = [acc * 100 for acc in test_acc]
        ax2.plot(rounds, test_acc_pct, label='Test Accuracy (%)',
                 marker='o', linewidth=2, color='green')
        ax2.set_xlabel('Round', fontweight='bold')
        ax2.set_ylabel('Accuracy (%)', fontweight='bold')
        ax2.set_title('Äá»™ chÃ­nh xÃ¡c trÃªn Test Set ToÃ n cá»¥c', fontweight='bold', fontsize=14)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim([0, 100])
        ax2.set_xticks(rounds)

        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"âœ… ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ lá»‹ch sá»­ huáº¥n luyá»‡n táº¡i: {save_path}")
        plt.show()

    except Exception as e:
        print(f"âš ï¸ Lá»—i khi váº½ biá»ƒu Ä‘á»“: {e}")
    finally:
        plt.close()
        print("="*80)


def evaluate_and_save_results(server, history, config, output_dir, data_stats, training_duration, start_time, end_time):
    """
    ÄÃ¡nh giÃ¡ cuá»‘i cÃ¹ng, lÆ°u model, history, config vÃ  táº¡o cÃ¡c bÃ¡o cÃ¡o.
    """
    print("\n" + "="*80)
    print("ğŸ’¾ BÆ¯á»šC 9 & 10: ÄÃNH GIÃ CUá»I CÃ™NG VÃ€ LÆ¯U Káº¾T QUáº¢")
    print("="*80)

    model_path = os.path.join(output_dir, 'global_model.pth')
    torch.save(server.global_model.state_dict(), model_path)
    print(f"âœ… ÄÃ£ lÆ°u mÃ´ hÃ¬nh toÃ n cá»¥c: {model_path}")

    history_path = os.path.join(output_dir, 'training_history.pkl')
    with open(history_path, 'wb') as f:
        pickle.dump(history, f)
    print(f"âœ… ÄÃ£ lÆ°u lá»‹ch sá»­ huáº¥n luyá»‡n: {history_path}")

    config_path = os.path.join(output_dir, 'config.json')
    config_to_save = config.copy()
    config_to_save['device'] = str(config['device'])
    with open(config_path, 'w') as f:
        json.dump(config_to_save, f, indent=2)
    print(f"âœ… ÄÃ£ lÆ°u cáº¥u hÃ¬nh: {config_path}")

    stats_path = os.path.join(output_dir, 'data_statistics.json')
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(data_stats, f, indent=2, ensure_ascii=False)
    print(f"âœ… ÄÃ£ lÆ°u thá»‘ng kÃª dá»¯ liá»‡u: {stats_path}")

    plot_path = os.path.join(output_dir, 'training_history.png')
    plot_training_history(history, plot_path)

    print("\nâ†’ Äang táº¡o dá»± Ä‘oÃ¡n (predictions) trÃªn Test Set toÃ n cá»¥c...")
    all_y_true = []
    all_y_pred = []
    server.global_model.eval()

    for loader_idx, loader in enumerate(server.client_test_loaders):
        pbar = tqdm(
            loader,
            desc=f"[Predict] Client TestLoader {loader_idx}",
            unit="batch",
            leave=False
        )
        with torch.no_grad():
            for data, target in pbar:
                data, target = data.to(server.device), target.to(server.device)
                output = server.global_model(data)
                pred = output.argmax(dim=1)

                all_y_true.append(target.cpu().numpy())
                all_y_pred.append(pred.cpu().numpy())

    y_true = np.concatenate(all_y_true)
    y_pred = np.concatenate(all_y_pred)
    print("âœ… ÄÃ£ táº¡o dá»± Ä‘oÃ¡n xong.")

    print("\n" + "="*80)
    print("ğŸ“„ CLASSIFICATION REPORT")
    print("="*80)
    class_labels = [str(i) for i in range(config['num_classes'])]
    report = classification_report(
        y_true,
        y_pred,
        labels=range(config['num_classes']),
        target_names=class_labels,
        zero_division=0,
        digits=4
    )
    print(report)

    report_path = os.path.join(output_dir, "classification_report.txt")
    with open(report_path, 'w') as f:
        f.write("CLASSIFICATION REPORT\n" + "="*80 + "\n\n" + report)
    print(f"\nğŸ’¾ ÄÃ£ lÆ°u report: {report_path}")

    print("\nâ†’ Äang táº¡o ma tráº­n nháº§m láº«n (Confusion Matrix)...")
    cm = confusion_matrix(y_true, y_pred, labels=range(config['num_classes']))

    show_labels = (config['num_classes'] <= 40)
    fig_size = max(12, config['num_classes'] * 0.4)
    plt.figure(figsize=(fig_size, fig_size * 0.8))

    sns.heatmap(
        cm,
        annot=show_labels,
        fmt='d',
        cmap='Blues',
        cbar=True,
        xticklabels=class_labels if show_labels else False,
        yticklabels=class_labels if show_labels else False
    )

    final_test_accuracy = history['test_accuracy'][-1]

    plt.title(f'Confusion Matrix - Final Global Model\n'
              f'Test Accuracy: {final_test_accuracy:.4f} ({final_test_accuracy*100:.2f}%)',
              fontsize=14, fontweight='bold')
    plt.xlabel('Predicted Label', fontsize=12)
    plt.ylabel('True Label', fontsize=12)

    if show_labels:
        plt.xticks(rotation=90)
        plt.yticks(rotation=0)

    plt.tight_layout()
    cm_path = os.path.join(output_dir, "confusion_matrix.png")
    plt.savefig(cm_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ÄÃ£ lÆ°u confusion matrix: {cm_path}")
    plt.show()
    plt.close()

    print("\nâ†’ Äang lÆ°u metrics chi tiáº¿t (F1, Precision, Recall)...")
    precision, recall, f1, support = precision_recall_fscore_support(
        y_true,
        y_pred,
        labels=range(config['num_classes']),
        average=None,
        zero_division=0
    )
    detailed_metrics = {
        'class': class_labels,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'support': support
    }
    df_metrics = pd.DataFrame(detailed_metrics)
    csv_path = os.path.join(output_dir, "detailed_metrics.csv")
    df_metrics.to_csv(csv_path, index=False)
    print(f"âœ… ÄÃ£ lÆ°u detailed metrics: {csv_path}")

    print("\n" + "="*80)
    print("ğŸ“Š TOP 5 CLASSES PERFORMANCE:")
    print("="*80)
    df_sorted = df_metrics.sort_values('f1_score', ascending=False)
    print("\nTop 5 Best Classes (by F1-score):")
    print(df_sorted.head(5).to_string(index=False))
    print("\nTop 5 Worst Classes (by F1-score):")
    print(df_sorted.tail(5).to_string(index=False))
    print("="*80)

    print("\n" + "="*80)
    print("ğŸ“ BÆ¯á»šC 10: Táº O SUMMARY REPORT")
    print("="*80)
    summary_path = os.path.join(OUTPUT_DIR, "SUMMARY_REPORT.txt")
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n" + " " * 20 + "FEDERATED LEARNING SUMMARY REPORT\n" +
                "="*80 + "\n\n")
        f.write("ğŸ“… THá»œI GIAN:\n")
        f.write(f"  â€¢ Báº¯t Ä‘áº§u: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"  â€¢ Káº¿t thÃºc: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"  â€¢ Tá»•ng thá»i gian: {training_duration:.2f}s ({training_duration/60:.2f} phÃºt)\n\n")
        f.write("âš™ï¸  Cáº¤U HÃŒNH:\n")
        f.write(f"  â€¢ Chiáº¿n lÆ°á»£c: {config['algorithm'].upper()}\n")
        if config['algorithm'] == 'fedprox':
            f.write(f"  â€¢ Mu (proximal): {config['mu']}\n")
        f.write(f"  â€¢ Sá»‘ clients: {config['num_clients']}\n")
        f.write(f"  â€¢ Sá»‘ rounds: {config['num_rounds']}\n")
        f.write(f"  â€¢ Epochs/round: {config['local_epochs']}\n")
        f.write(f"  â€¢ Batch size: {config['batch_size']}\n")
        f.write(f"  â€¢ Learning rate: {config['learning_rate']}\n")
        f.write(f"  â€¢ Input features: {config['input_shape'][0]}\n")
        f.write(f"  â€¢ Num classes: {config['num_classes']}\n")
        f.write(f"  â€¢ Cháº¡y song song: {config['use_multiprocessing']}\n")

        f.write("\nğŸ“Š Káº¾T QUáº¢ CUá»I CÃ™NG (Tá»”NG Há»¢P Tá»ª TEST SET):\n")
        if history['test_accuracy']:
            final_acc = history['test_accuracy'][-1]
            final_loss = history['test_loss'][-1]
            f.write(f"  â€¢ Final Test Accuracy: {final_acc:.4f} ({final_acc*100:.2f}%)\n")
            f.write(f"  â€¢ Final Test Loss: {final_loss:.4f}\n")

        f.write("\nğŸ“ OUTPUT FILES:\n")
        f.write(f"  â€¢ ThÆ° má»¥c: {OUTPUT_DIR}\n")
        f.write(f"  â€¢ Model: global_model.pth\n")
        f.write(f"  â€¢ History: training_history.pkl\n")
        f.write(f"  â€¢ Plots: training_history.png\n")
        f.write(f"  â€¢ Report: classification_report.txt\n")
        f.write(f"  â€¢ Metrics: detailed_metrics.csv\n")
        f.write(f"  â€¢ Config: config.json\n")

        f.write("\n" + "="*80 + "\n" + "âœ… HUáº¤N LUYá»†N THÃ€NH CÃ”NG!\n" + "="*80 + "\n")

    print(f"âœ… ÄÃ£ táº¡o summary report: {summary_path}")


# ============================================================================
# ğŸ’¡ BÆ¯á»šC 11: HÃ€M MAIN ğŸ’¡
# ============================================================================

def main():
    config = CONFIG
    start_time = datetime.now()

    print("="*80)
    print("ğŸ¤– FEDERATED LEARNING Vá»šI MÃ” HÃŒNH CNN-GRU (IoT IDS)")
    print("="*80)

    try:
        device = check_and_setup_gpu(config)
        config['device'] = device

        input_shape, num_classes, data_stats = auto_detect_data_parameters(
            data_dir=config['data_dir'],
            num_clients=config['num_clients']
        )
        config['input_shape'] = input_shape
        config['num_classes'] = num_classes

        print("\n" + "="*80)
        print("âš™ï¸  Cáº¤U HÃŒNH CUá»I CÃ™NG")
        print("="*80)
        print(json.dumps(config, indent=2, default=str))
        print("="*80)

        train_loaders, test_loaders = load_federated_data(
            data_dir=config['data_dir'],
            num_clients=config['num_clients'],
            batch_size=config['batch_size'],
            device=config['device']
        )

        server, clients = initialize_federated_system(
            train_loaders=train_loaders,
            test_loaders=test_loaders,
            input_shape=config['input_shape'],
            num_classes=config['num_classes'],
            device=config['device']
        )

        history = train_federated(server, config, train_loaders)

        end_time = datetime.now()
        training_duration = (end_time - start_time).total_seconds()

        print("\n" + "="*80)
        print("ğŸ HUáº¤N LUYá»†N HOÃ€N Táº¤T!")
        print("="*80)
        print(f"â±ï¸  Thá»i gian huáº¥n luyá»‡n: {training_duration:.2f} giÃ¢y ({training_duration/60:.2f} phÃºt)")

        evaluate_and_save_results(
            server, history, config,
            config['output_dir'], data_stats,
            training_duration, start_time, end_time
        )

        print("\n" + "="*80)
        print("ğŸ‰ ğŸ‰ ğŸ‰  HOÃ€N Táº¤T Táº¤T Cáº¢ CÃC BÆ¯á»šC  ğŸ‰ ğŸ‰ ğŸ‰")
        print("="*80)

    except Exception as e:
        logger.error("\nâŒ ÄÃƒ Xáº¢Y RA Lá»–I TRONG QUÃ TRÃŒNH CHáº Y SCRIPT")
        logger.error(f"Chi tiáº¿t lá»—i: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
