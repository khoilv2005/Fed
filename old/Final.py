################################################################################
#                                                                              #
#  SCRIPT HUáº¤N LUYá»†N FEDERATED LEARNING (FLOWER + PYTORCH)                      #
#  Framework: Flower (Tá»± Ä‘á»™ng dÃ¹ng Ray cho cháº¡y song song)                      #
#  MÃ´ hÃ¬nh: ğŸŒŸ PyTorch CNN-GRU (Tá»± code) ğŸŒŸ                                  #
#  Chiáº¿n lÆ°á»£c: ğŸŒŸ FedProx (Cá»§a Flower) ğŸŒŸ                                      #
#  TÃ­nh nÄƒng: âš¡ Tá»I Æ¯U GPU (Tá»± Ä‘á»™ng) + BÃO CÃO F1-Score                         #
#                                                                              #
################################################################################

import flwr as fl
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, TensorDataset
import numpy as np
import os
import logging
import pickle
import json
import copy
from collections import OrderedDict
from typing import List, Dict, Tuple, Optional
from datetime import datetime
from tqdm.auto import tqdm
import time
import torch.multiprocessing as mp # Import Ä‘á»ƒ set 'spawn'

# === THÃŠM THÆ¯ VIá»†N CHO BIá»‚U Äá»’ VÃ€ Káº¾T QUáº¢ ===
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    confusion_matrix, 
    classification_report, 
    precision_recall_fscore_support
)
import pandas as pd
# ============================================

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

# ============================================================================
# ğŸ’¡ BÆ¯á»šC 1: Cáº¤U HÃŒNH CHÃNH ğŸ’¡
# ============================================================================

CONFIG = {
    # â¬‡ï¸ CHá»ˆNH ÄÆ¯á»œNG DáºªN NÃ€Y (TRÃŠN COLAB Cáº¦N MOUNT DRIVE TRÆ¯á»šC) â¬‡ï¸
    'data_dir': '/kaggle/input/fed-5clients',
    'output_dir': './results',
    
    'num_clients': 5,

    # Model params (sáº½ Ä‘Æ°á»£c tá»± Ä‘á»™ng phÃ¡t hiá»‡n tá»« data)
    'input_shape': None,  
    'num_classes': None,  

    # Training params
    'algorithm': 'fedprox',     # 'fedavg' hoáº·c 'fedprox'
    'num_rounds': 1,            # 1 VÃ²ng
    'local_epochs': 1,          # 1 Epoch/VÃ²ng
    'learning_rate': 0.001,
    'batch_size': 1024,         
    'client_fraction': 1.0,     # 1.0 = chá»n táº¥t cáº£ 5 client

    # FedProx specific
    'mu': 0.01,  # Proximal term coefficient

    # Device
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'force_gpu': True,
    
    # Visualization
    'eval_every': 1,  # ÄÃ¡nh giÃ¡ sau má»—i round
}

# === Táº O THÆ¯ Má»¤C OUTPUT ===
TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")
OUTPUT_DIR = os.path.join(CONFIG['output_dir'], f"run_{TIMESTAMP}_{CONFIG['algorithm']}")
os.makedirs(OUTPUT_DIR, exist_ok=True)
CONFIG['output_dir'] = OUTPUT_DIR 


# ============================================================================
# ğŸ’¡ BÆ¯á»šC 2: Äá»ŠNH NGHÄ¨A MÃ” HÃŒNH CNN-GRU (PyTorch) ğŸ’¡
# (Ná»™i dung tá»« model.py cá»§a báº¡n)
# ============================================================================

class CNN_GRU_Model(nn.Module):
    def __init__(self, input_shape, num_classes=2):
        super(CNN_GRU_Model, self).__init__()

        if isinstance(input_shape, tuple):
            seq_length = input_shape[0]
        else:
            seq_length = input_shape

        self.input_shape = input_shape
        self.num_classes = num_classes
        
        def conv_output_shape(L_in, kernel_size=1, stride=1, padding=0, dilation=1):
            if padding == 1 and kernel_size == 3: L_out_conv = L_in
            else: L_out_conv = (L_in + 2*padding - dilation*(kernel_size-1) - 1) // stride + 1
            return L_out_conv
        def pool_output_shape(L_in, kernel_size=2, stride=2, padding=0, dilation=1):
            return (L_in + 2*padding - dilation*(kernel_size-1) - 1) // stride + 1

        # ===== CNN MODULE =====
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm1d(64)
        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)
        self.dropout_cnn1 = nn.Dropout(0.2)
        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm1d(128)
        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)
        self.dropout_cnn2 = nn.Dropout(0.2)
        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm1d(256)
        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)
        self.dropout_cnn3 = nn.Dropout(0.3)

        cnn_output_length = seq_length
        cnn_output_length = pool_output_shape(conv_output_shape(cnn_output_length, kernel_size=3, padding=1))
        cnn_output_length = pool_output_shape(conv_output_shape(cnn_output_length, kernel_size=3, padding=1))
        cnn_output_length = pool_output_shape(conv_output_shape(cnn_output_length, kernel_size=3, padding=1))
        self.cnn_output_size = 256 * cnn_output_length
        
        # ===== GRU MODULE =====
        self.gru1 = nn.GRU(input_size=1, hidden_size=128, batch_first=True)
        self.gru2 = nn.GRU(input_size=128, hidden_size=64, batch_first=True)
        self.gru_output_size = 64

        # ===== MLP MODULE =====
        concat_size = self.cnn_output_size + self.gru_output_size
        self.dense1 = nn.Linear(concat_size, 256)
        self.bn_mlp1 = nn.BatchNorm1d(256)
        self.dropout1 = nn.Dropout(0.4)
        self.dense2 = nn.Linear(256, 128)
        self.bn_mlp2 = nn.BatchNorm1d(128)
        self.dropout2 = nn.Dropout(0.3)
        self.output = nn.Linear(128, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        if len(x.shape) == 2: x = x.unsqueeze(-1)
        batch_size = x.size(0); x_cnn = x.permute(0, 2, 1)
        x_cnn = self.dropout_cnn1(self.pool1(self.relu(self.bn1(self.conv1(x_cnn)))))
        x_cnn = self.dropout_cnn2(self.pool2(self.relu(self.bn2(self.conv2(x_cnn)))))
        x_cnn = self.dropout_cnn3(self.pool3(self.relu(self.bn3(self.conv3(x_cnn)))))
        cnn_output = x_cnn.view(batch_size, -1); x_gru = x; x_gru, _ = self.gru1(x_gru); x_gru, _ = self.gru2(x_gru)
        gru_output = x_gru[:, -1, :]; concatenated = torch.cat([cnn_output, gru_output], dim=1)
        x = self.dense1(concatenated); 
        if x.shape[0] > 1: x = self.bn_mlp1(x)
        x = self.relu(x); x = self.dropout1(x)
        x = self.dense2(x); 
        if x.shape[0] > 1: x = self.bn_mlp2(x)
        x = self.relu(x); x = self.dropout2(x)
        return self.output(x)

def build_cnn_gru_model(input_shape, num_classes=2):
    """HÃ m tiá»‡n Ã­ch Ä‘á»ƒ khá»Ÿi táº¡o model CNN-GRU."""
    model = CNN_GRU_Model(input_shape, num_classes)
    logger.info(f"âœ… Khá»Ÿi táº¡o mÃ´ hÃ¬nh CNN-GRU (PyTorch) thÃ nh cÃ´ng")
    logger.info(f"   - KÃ­ch thÆ°á»›c input: {input_shape}")
    logger.info(f"   - Sá»‘ lá»›p (num_classes): {num_classes}")
    return model

# ============================================================================
# ğŸ’¡ BÆ¯á»šC 3: HÃ€M LOAD Dá»® LIá»†U ğŸ’¡
# ============================================================================

class NumpyDataset(TensorDataset):
    """Dataset tiá»‡n dá»¥ng Ä‘á»ƒ wrap numpy array thÃ nh TensorDataset."""
    def __init__(self, X, y):
        if len(X.shape) == 3: X = X.squeeze(-1)
        X = X.astype(np.float32)
        X_tensor = torch.from_numpy(X)
        y_tensor = torch.from_numpy(y).long()
        super().__init__(X_tensor, y_tensor)

def load_data_for_client(data_dir, client_id, batch_size):
    """
    Load data CHá»ˆ CHO 1 client (dÃ¹ng trong client_fn)
    """
    data_path = os.path.join(data_dir, f'client_{client_id}_data.npz')
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u cá»§a client {client_id} táº¡i: {data_path}")
        
    data = np.load(data_path)
    X_train = data['X_train']; y_train = data['y_train']
    X_test = data['X_test']; y_test = data['y_test']
    
    train_dataset = NumpyDataset(X_train, y_train)
    test_dataset = NumpyDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False)
    
    logger.info(f"   - [client_fn(cid={client_id})] ÄÃ£ load {len(train_dataset):,} train, {len(test_dataset):,} test.")
    return train_loader, test_loader, len(train_dataset)

def load_global_test_set(data_dir, num_clients, batch_size):
    """
    âœ… Sá»¬A Lá»–I RAM OOM: Táº¡o 1 DataLoader gá»™p (dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡)
    """
    logger.info("\nâ†’ Táº¡o global test loader (gá»™p test cá»§a táº¥t cáº£ client)...")
    all_X_test = []
    all_y_test = []
    
    for client_id in range(num_clients):
        data_path = os.path.join(data_dir, f'client_{client_id}_data.npz')
        with np.load(data_path) as data:
            all_X_test.append(data['X_test'])
            all_y_test.append(data['y_test'])
        
    X_test_global = np.concatenate(all_X_test, axis=0)
    y_test_global = np.concatenate(all_y_test, axis=0)
    
    global_test_dataset = NumpyDataset(X_test_global, y_test_global)
    global_test_loader = DataLoader(global_test_dataset, batch_size=batch_size * 2, shuffle=False)
    
    logger.info(f"   - KÃ­ch thÆ°á»›c global test set: {len(global_test_dataset):,} máº«u.")
    return global_test_loader

# ============================================================================
# ğŸ’¡ BÆ¯á»šC 4: Äá»ŠNH NGHÄ¨A FLOWER CLIENT (PyTorch) ğŸ’¡
# (ÄÃ¢y lÃ  logic "tá»± code" cá»§a báº¡n, Ä‘Æ°á»£c bá»c trong Flower)
# ============================================================================

class FlowerClient(fl.client.NumPyClient):
    def __init__(self, cid, model, trainloader, testloader, device):
        self.cid = cid
        self.model = model
        self.trainloader = trainloader
        self.testloader = testloader
        self.device = device

    def get_parameters(self, config):
        """Láº¥y tham sá»‘ model (weights) vÃ  chuyá»ƒn sang NumPy"""
        # logger.info(f"[Client {self.cid}] Äang gá»­i tham sá»‘ (get_parameters)")
        # Chuyá»ƒn state_dict (OrderedDict) thÃ nh list cÃ¡c máº£ng NumPy
        return [val.cpu().numpy() for val in self.model.state_dict().values()]

    def set_parameters(self, parameters):
        """Nháº­n tham sá»‘ (weights) tá»« server vÃ  cáº­p nháº­t model"""
        # logger.info(f"[Client {self.cid}] Äang nháº­n tham sá»‘ (set_parameters)")
        # Chuyá»ƒn list máº£ng NumPy trá»Ÿ láº¡i thÃ nh state_dict
        params_dict = zip(self.model.state_dict().keys(), parameters)
        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})
        self.model.load_state_dict(state_dict, strict=True)

    def fit(self, parameters, config):
        """
        Huáº¥n luyá»‡n model trÃªn dá»¯ liá»‡u local (FedAvg hoáº·c FedProx)
        """
        logger.info(f"[Client {self.cid}] Báº¯t Ä‘áº§u huáº¥n luyá»‡n (fit)...")
        self.set_parameters(parameters) # Nháº­n model má»›i tá»« server
        
        # Láº¥y tham sá»‘ tá»« server
        algorithm = config.get('algorithm', 'fedavg')
        epochs = config.get('local_epochs', 1)
        learning_rate = config.get('learning_rate', 0.001)
        mu = config.get('mu', 0.01)

        self.model.train()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        
        # LÆ°u global params (náº¿u lÃ  FedProx)
        global_params_dict = None
        if algorithm == 'fedprox':
            # âœ… Sá»¬A Lá»–I: Sá»­ dá»¥ng state_dict().keys() Ä‘á»ƒ mapping Ä‘Ãºng vá»›i parameters
            params_dict = zip(self.model.state_dict().keys(), parameters)
            global_params_dict = {
                k: torch.tensor(v).to(self.device)
                for k, v in params_dict
            }

        for epoch in range(epochs):
            epoch_loss = 0.0
            epoch_samples = 0
            
            pbar = tqdm(
                self.trainloader,
                desc=f"[Client {self.cid}] Epoch {epoch+1}/{epochs}",
                unit="batch",
                leave=False,
                position=int(self.cid) # Vá»‹ trÃ­ thanh progress
            )
            
            for data, target in pbar:
                data, target = data.to(self.device), target.to(self.device)
                
                optimizer.zero_grad()
                output = self.model(data)
                
                ce_loss = criterion(output, target)
                loss = ce_loss # Máº·c Ä‘á»‹nh lÃ  FedAvg

                if algorithm == 'fedprox':
                    # === âœ… Sá»¬A Lá»–I: Logic "tá»± code" FedProx ===
                    proximal_term = 0.0
                    for name, param in self.model.named_parameters():
                        if param.requires_grad:
                            global_param = global_params_dict[name]
                            proximal_term += torch.sum((param - global_param) ** 2)
                    
                    proximal_term = (mu / 2) * proximal_term
                    loss += proximal_term
                
                loss.backward()
                optimizer.step()
                
                epoch_loss += ce_loss.item() * data.size(0) # Chá»‰ log CE loss
                epoch_samples += data.size(0)
                
                pbar.set_postfix({
                    "ce_loss": f"{ce_loss.item():.4f}",
                    "loss": f"{loss.item():.4f}"
                })

            avg_loss = epoch_loss / max(1, epoch_samples)
            # KhÃ´ng in log á»Ÿ Ä‘Ã¢y, Ä‘á»ƒ pbar tá»± xá»­ lÃ½

        # Tráº£ vá» model Ä‘Ã£ huáº¥n luyá»‡n (dÆ°á»›i dáº¡ng NumPy) vÃ  sá»‘ máº«u
        return self.get_parameters(config={}), len(self.trainloader.dataset), {"avg_loss": avg_loss}

    def evaluate(self, parameters, config):
        """
        Flower sáº½ KHÃ”NG gá»i hÃ m nÃ y náº¿u 'evaluate_fn' Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a á»Ÿ server
        """
        return 0.0, 0, {"accuracy": 0.0}

# ============================================================================
# ğŸ’¡ BÆ¯á»šC 5: HÃ€M Tá»° Äá»˜NG PHÃT HIá»†N THAM Sá» Dá»® LIá»†U ğŸ’¡
# ============================================================================

def auto_detect_data_parameters(data_dir, num_clients):
    logger.info("\n" + "="*80)
    logger.info("ğŸ“‚ Tá»° Äá»˜NG PHÃT HIá»†N THAM Sá» Dá»® LIá»†U")
    logger.info("="*80)
    logger.info(f"â†’ ThÆ° má»¥c dá»¯ liá»‡u: {data_dir}")
    logger.info(f"â†’ Sá»‘ lÆ°á»£ng client (dá»± kiáº¿n): {num_clients}")

    try:
        all_labels = []
        data_stats = {}
        client_0_path = os.path.join(data_dir, "client_0_data.npz")
        if not os.path.exists(client_0_path):
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file: {client_0_path}")

        with np.load(client_0_path) as data:
            x_train_sample = data['X_train']
            input_features = x_train_sample.shape[1]
            input_shape = (input_features,)
            logger.info(f"\nâœ… ThÃ´ng tin tá»« client 0:")
            logger.info(f"   - Sá»‘ Ä‘áº·c trÆ°ng (INPUT_FEATURES): {input_features}")
            logger.info(f"   - input_shape: {input_shape}")

        logger.info(f"\nâ†’ Äang quÃ©t dá»¯ liá»‡u cá»§a {num_clients} client Ä‘á»ƒ thá»‘ng kÃª nhÃ£n...")
        total_train = 0
        total_test = 0

        for i in range(num_clients):
            file_path = os.path.join(data_dir, f"client_{i}_data.npz")
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file: {file_path}")

            with np.load(file_path) as data:
                x_train = data['X_train']; y_train = data['y_train']
                x_test = data['X_test']; y_test = data['y_test']
                all_labels.append(y_train); all_labels.append(y_test) # QuÃ©t cáº£ train vÃ  test
                unique_labels, counts = np.unique(y_train, return_counts=True)
                total_train += len(x_train)
                total_test += len(x_test)
                data_stats[i] = {
                    'train_samples': int(len(x_train)),
                    'test_samples': int(len(x_test)),
                    'unique_labels': int(len(unique_labels)),
                    'label_distribution': {str(k): int(v) for k, v in zip(unique_labels, counts)}
                }
                logger.info(f"   - Client {i}: {len(x_train):,} train, {len(x_test):,} test, {len(unique_labels)} nhÃ£n")

        combined_labels = np.concatenate(all_labels)
        num_classes = len(np.unique(combined_labels))

        logger.info("\nğŸ“Š Tá»•ng há»£p toÃ n bá»™ dá»¯ liá»‡u:")
        logger.info(f"   - Sá»‘ lá»›p (num_classes): {num_classes}")
        logger.info(f"   - Tá»•ng sá»‘ máº«u train: {total_train:,}")
        logger.info(f"   - Tá»•ng sá»‘ máº«u test:  {total_test:,}")
        logger.info("="*80)

        return input_shape, num_classes, data_stats

    except FileNotFoundError as e:
        logger.error("\n" + "="*80 + f"\nâŒ Lá»–I: KHÃ”NG TÃŒM THáº¤Y Tá»†P Dá»® LIá»†U\nÄÆ°á»ng dáº«n: {e.filename}\n" + "="*80)
        raise
    except KeyError as e:
        logger.error("\n" + "="*80 + f"\nâŒ Lá»–I: THIáº¾U KEY TRONG FILE .NPZ\nKey: {e}\n" + "="*80)
        raise

# ============================================================================
# ğŸ’¡ BÆ¯á»šC 6: HÃ€M Váº¼ BIá»‚U Äá»’ & LÆ¯U Káº¾T QUáº¢ ğŸ’¡
# ============================================================================

def plot_training_history(history, save_path):
    """
    Váº½ biá»ƒu Ä‘á»“ train_loss, test_loss vÃ  test_accuracy theo round.
    """
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š ÄANG Váº¼ BIá»‚U Äá»’ Káº¾T QUáº¢ HUáº¤N LUYá»†N")
    logger.info("="*80)

    try:
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        # Láº¥y dá»¯ liá»‡u tá»« history.losses_distributed (list of tuples (round, loss))
        # Bá» qua round 0 náº¿u cÃ³ (vÃ¬ ta cÃ³ evaluate_fn á»Ÿ round 0)
        if history.losses_distributed[0][0] == 0:
             rounds = [r for r, _ in history.losses_distributed][1:]
             train_loss = [l for _, l in history.losses_distributed][1:]
        else:
             rounds = [r for r, _ in history.losses_distributed]
             train_loss = [l for _, l in history.losses_distributed]
        
        # Láº¥y dá»¯ liá»‡u tá»« history.metrics_centralized (dict of lists)
        # Bá» qua round 0 (init)
        rounds_eval = [r for r, _ in history.metrics_centralized['accuracy']]
        test_acc = [a for _, a in history.metrics_centralized['accuracy']]
        test_loss = [l for _, l in history.metrics_centralized['test_loss']]


        # Loss
        ax1 = axes[0]
        ax1.plot(rounds, train_loss, label='Train Loss (Trung bÃ¬nh Client)', marker='o', linewidth=2)
        ax1.plot(rounds_eval, test_loss, label='Test Loss (ToÃ n cá»¥c)', marker='s', linewidth=2)
        ax1.set_xlabel('Round', fontweight='bold')
        ax1.set_ylabel('Loss', fontweight='bold')
        ax1.set_title('Train Loss & Test Loss', fontweight='bold', fontsize=14)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_xticks(rounds_eval)

        # Accuracy
        ax2 = axes[1]
        test_acc_pct = [acc * 100 for acc in test_acc]
        ax2.plot(rounds_eval, test_acc_pct, label='Test Accuracy (%)', marker='o', linewidth=2, color='green')
        ax2.set_xlabel('Round', fontweight='bold')
        ax2.set_ylabel('Accuracy (%)', fontweight='bold')
        ax2.set_title('Äá»™ chÃ­nh xÃ¡c trÃªn Test Set ToÃ n cá»¥c', fontweight='bold', fontsize=14)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim([0, 100])
        ax2.set_xticks(rounds_eval)

        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        logger.info(f"âœ… ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ lá»‹ch sá»­ huáº¥n luyá»‡n táº¡i: {save_path}")
        plt.show() # Hiá»ƒn thá»‹ plot trong notebook
    
    except Exception as e:
        logger.warning(f"âš ï¸ Lá»—i khi váº½ biá»ƒu Ä‘á»“: {e}")
        logger.warning(f"History object (metrics_centralized): {history.metrics_centralized}")
        logger.warning(f"History object (losses_distributed): {history.losses_distributed}")
    finally:
        plt.close() # Äáº£m báº£o Ä‘Ã³ng plot
        logger.info("="*80)


def evaluate_and_save_results(
    server_model, history, config, output_dir, data_stats, 
    training_duration, start_time, end_time
):
    """
    ÄÃ¡nh giÃ¡ cuá»‘i cÃ¹ng, lÆ°u model, history, config vÃ  táº¡o cÃ¡c bÃ¡o cÃ¡o.
    """
    logger.info("\n" + "="*80)
    logger.info("ğŸ’¾ BÆ¯á»šC 9 & 10: ÄÃNH GIÃ CUá»I CÃ™NG VÃ€ LÆ¯U Káº¾T QUáº¢")
    logger.info("="*80)
    
    device = config['device']

    # 1. LÆ°u Model
    model_path = os.path.join(output_dir, 'global_model.pth')
    torch.save(server_model.state_dict(), model_path)
    logger.info(f"âœ… ÄÃ£ lÆ°u mÃ´ hÃ¬nh toÃ n cá»¥c: {model_path}")

    # 2. LÆ°u History (tá»« Flower)
    history_path = os.path.join(output_dir, 'training_history.pkl')
    with open(history_path, 'wb') as f:
        pickle.dump(history, f)
    logger.info(f"âœ… ÄÃ£ lÆ°u lá»‹ch sá»­ huáº¥n luyá»‡n: {history_path}")

    # 3. LÆ°u Config
    config_path = os.path.join(output_dir, 'config.json')
    config_to_save = config.copy()
    config_to_save['device'] = str(config['device'])
    config_to_save['input_shape'] = str(config['input_shape']) 
    with open(config_path, 'w') as f:
        json.dump(config_to_save, f, indent=2)
    logger.info(f"âœ… ÄÃ£ lÆ°u cáº¥u hÃ¬nh: {config_path}")

    # 4. LÆ°u Thá»‘ng kÃª Dá»¯ liá»‡u
    stats_path = os.path.join(output_dir, 'data_statistics.json')
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(data_stats, f, indent=2, ensure_ascii=False)
    logger.info(f"âœ… ÄÃ£ lÆ°u thá»‘ng kÃª dá»¯ liá»‡u: {stats_path}")

    # 5. Váº½ Biá»ƒu Ä‘á»“
    plot_path = os.path.join(output_dir, 'training_history.png')
    plot_training_history(history, plot_path)
    
    # 6. ÄÃ¡nh giÃ¡ cuá»‘i cÃ¹ng (Láº¥y dá»± Ä‘oÃ¡n)
    logger.info("\nâ†’ Äang táº¡o dá»± Ä‘oÃ¡n (predictions) trÃªn Test Set toÃ n cá»¥c...")
    global_test_loader = load_global_test_set(
        config['data_dir'], config['num_clients'], config['batch_size']
    )
    
    all_y_true = []
    all_y_pred = []
    server_model.to(device) # Äáº£m báº£o model trÃªn Ä‘Ãºng device
    server_model.eval()
    
    pbar_predict = tqdm(
        global_test_loader,
        desc="[Predict] Láº¥y dá»± Ä‘oÃ¡n tá»« Test Set",
        unit="batch",
        leave=False
    )

    with torch.no_grad():
        for data, target in pbar_predict:
            data, target = data.to(device), target.to(device)
            output = server_model(data)
            pred = output.argmax(dim=1)
            
            all_y_true.append(target.cpu().numpy())
            all_y_pred.append(pred.cpu().numpy())
                
    y_true = np.concatenate(all_y_true)
    y_pred = np.concatenate(all_y_pred)
    logger.info("âœ… ÄÃ£ táº¡o dá»± Ä‘oÃ¡n xong.")

    # 7. In vÃ  LÆ°u BÃ¡o cÃ¡o PhÃ¢n loáº¡i
    logger.info("\n" + "="*80)
    logger.info("ğŸ“„ CLASSIFICATION REPORT")
    logger.info("="*80)
    class_labels = [str(i) for i in range(config['num_classes'])]
    report = classification_report(
        y_true, 
        y_pred, 
        labels=range(config['num_classes']),
        target_names=class_labels,
        zero_division=0,
        digits=4
    )
    print(report) # In ra mÃ n hÃ¬nh
    
    report_path = os.path.join(output_dir, "classification_report.txt")
    with open(report_path, 'w') as f:
        f.write("CLASSIFICATION REPORT\n" + "="*80 + "\n\n" + report)
    logger.info(f"\nğŸ’¾ ÄÃ£ lÆ°u report: {report_path}")

    # 8. Váº½ vÃ  LÆ°u Ma tráº­n Nháº§m láº«n
    logger.info("\nâ†’ Äang táº¡o ma tráº­n nháº§m láº«n (Confusion Matrix)...")
    cm = confusion_matrix(y_true, y_pred, labels=range(config['num_classes']))
    
    show_labels = (config['num_classes'] <= 40)
    fig_size = max(12, config['num_classes'] * 0.4)
    plt.figure(figsize=(fig_size, fig_size * 0.8))
    
    sns.heatmap(
        cm, 
        annot=show_labels, 
        fmt='d', 
        cmap='Blues',
        cbar=True,
        xticklabels=class_labels if show_labels else False,
        yticklabels=class_labels if show_labels else False
    )
    
    final_test_accuracy = history.metrics_centralized['accuracy'][-1][1] # Láº¥y giÃ¡ trá»‹ acc cuá»‘i
    
    plt.title(f'Confusion Matrix - Final Global Model\n'
              f'Test Accuracy: {final_test_accuracy:.4f} ({final_test_accuracy*100:.2f}%)',
              fontsize=14, fontweight='bold')
    plt.xlabel('Predicted Label', fontsize=12)
    plt.ylabel('True Label', fontsize=12)
    
    if show_labels:
        plt.xticks(rotation=90); plt.yticks(rotation=0)
    
    plt.tight_layout()
    cm_path = os.path.join(output_dir, "confusion_matrix.png")
    plt.savefig(cm_path, dpi=300, bbox_inches='tight')
    logger.info(f"âœ… ÄÃ£ lÆ°u confusion matrix: {cm_path}")
    plt.show()
    plt.close()

    # 9. LÆ°u Metrics chi tiáº¿t (F1, Precision, Recall)
    logger.info("\nâ†’ Äang lÆ°u metrics chi tiáº¿t (F1, Precision, Recall)...")
    precision, recall, f1, support = precision_recall_fscore_support(
        y_true, 
        y_pred, 
        labels=range(config['num_classes']),
        average=None,
        zero_division=0
    )
    detailed_metrics = {
        'class': class_labels,
        'precision': precision, 'recall': recall, 'f1_score': f1, 'support': support
    }
    df_metrics = pd.DataFrame(detailed_metrics)
    csv_path = os.path.join(output_dir, "detailed_metrics.csv")
    df_metrics.to_csv(csv_path, index=False)
    logger.info(f"âœ… ÄÃ£ lÆ°u detailed metrics: {csv_path}")

    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š TOP 5 CLASSES PERFORMANCE:")
    logger.info("="*80)
    df_sorted = df_metrics.sort_values('f1_score', ascending=False)
    print("\nTop 5 Best Classes (by F1-score):")
    print(df_sorted.head(5).to_string(index=False))
    print("\nTop 5 Worst Classes (by F1-score):")
    print(df_sorted.tail(5).to_string(index=False))
    logger.info("="*80)
    
    # 10. Táº¡o Summary Report
    logger.info("\n" + "="*80)
    logger.info("ğŸ“ BÆ¯á»šC 10: Táº O SUMMARY REPORT")
    logger.info("="*80)
    summary_path = os.path.join(OUTPUT_DIR, "SUMMARY_REPORT.txt")
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n" + " "*20 + "FEDERATED LEARNING SUMMARY REPORT\n" + "="*80 + "\n\n")
        f.write("ğŸ“… THá»œI GIAN:\n")
        f.write(f"  â€¢ Báº¯t Ä‘áº§u: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"  â€¢ Káº¿t thÃºc: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"  â€¢ Tá»•ng thá»i gian: {training_duration:.2f}s ({training_duration/60:.2f} phÃºt)\n\n")
        f.write("âš™ï¸  Cáº¤U HÃŒNH:\n")
        f.write(f"  â€¢ Chiáº¿n lÆ°á»£c: {config['algorithm'].upper()}\n")
        if config['algorithm'] == 'fedprox':
            f.write(f"  â€¢ Mu (proximal): {config['mu']}\n")
        f.write(f"  â€¢ Sá»‘ clients: {config['num_clients']}\n")
        f.write(f"  â€¢ Sá»‘ rounds: {config['num_rounds']}\n")
        f.write(f"  â€¢ Epochs/round: {config['local_epochs']}\n")
        f.write(f"  â€¢ Batch size: {config['batch_size']}\n")
        f.write(f"  â€¢ Learning rate: {config['learning_rate']}\n")
        f.write(f"  â€¢ Input features: {config['input_shape'][0]}\n")
        f.write(f"  â€¢ Num classes: {config['num_classes']}\n")
        f.write(f"  â€¢ Cháº¡y song song: Flower (Ray)\n")
        
        f.write("\nğŸ“Š Káº¾T QUáº¢ CUá»I CÃ™NG (Tá»”NG Há»¢P Tá»ª TEST SET):\n")
        if history.metrics_centralized['accuracy']:
            final_acc = history.metrics_centralized['accuracy'][-1][1]
            final_loss = history.losses_distributed[-1][1]
            f.write(f"  â€¢ Final Test Accuracy: {final_acc:.4f} ({final_acc*100:.2f}%)\n")
            f.write(f"  â€¢ Final Test Loss: {final_loss:.4f}\n")
        
        f.write("\nğŸ“ OUTPUT FILES:\n")
        f.write(f"  â€¢ ThÆ° má»¥c: {OUTPUT_DIR}\n")
        f.write(f"  â€¢ Model: global_model.pth\n")
        f.write(f"  â€¢ History: training_history.pkl\n")
        f.write(f"  â€¢ Plots: training_history.png\n")
        f.write(f"  â€¢ Report: classification_report.txt\n")
        f.write(f"  â€¢ Metrics: detailed_metrics.csv\n")
        f.write(f"  â€¢ Config: config.json\n")
        
        f.write("\n" + "="*80 + "\n" + "âœ… HUáº¤N LUYá»†N THÃ€NH CÃ”NG!\n" + "="*80 + "\n")

    logger.info(f"âœ… ÄÃ£ táº¡o summary report: {summary_path}")

# ============================================================================
# ğŸ’¡ BÆ¯á»šC 11: HÃ€M MAIN (Äá»‚ CHáº Y) ğŸ’¡
# ============================================================================

def check_and_setup_gpu(config: Dict) -> str:
    """
    Kiá»ƒm tra GPU vÃ  tráº£ vá» torch.device phÃ¹ há»£p.
    """
    logger.info("\n" + "="*80)
    logger.info("ğŸ”§ KIá»‚M TRA THIáº¾T Bá»Š GPU/CPU")
    logger.info("="*80)

    cuda_available = torch.cuda.is_available()
    logger.info(f"   - CUDA kháº£ dá»¥ng: {cuda_available}")

    if cuda_available:
        device = torch.device('cuda')
        logger.info(f"   - PhiÃªn báº£n CUDA: {torch.version.cuda}")
        logger.info(f"   - Sá»‘ lÆ°á»£ng GPU: {torch.cuda.device_count()}")

        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            logger.info(f"\n   â¤ GPU {i}:")
            logger.info(f" Â  Â  Â - TÃªn: {torch.cuda.get_device_name(i)}")
            logger.info(f" Â  Â  Â - Bá»™ nhá»›: {props.total_memory / 1024**3:.2f} GB")
            logger.info(f" Â  Â  Â - Compute capability: {props.major}.{props.minor}")
        
        logger.info(f"\nâœ… Sá»­ dá»¥ng thiáº¿t bá»‹: {torch.cuda.get_device_name(0)}")
        # Tráº£ vá» string 'cuda'
        device_str = 'cuda'
        
    else:
        if config.get('force_gpu', False):
            logger.error("\nâŒ Lá»–I: KhÃ´ng phÃ¡t hiá»‡n GPU nhÆ°ng force_gpu=True")
            raise RuntimeError("YÃªu cáº§u GPU nhÆ°ng khÃ´ng cÃ³ GPU kháº£ dá»¥ng.")
        else:
            device = torch.device('cpu')
            logger.warning(f"\nâš ï¸ Cáº£nh bÃ¡o: KhÃ´ng cÃ³ GPU, há»‡ thá»‘ng sáº½ cháº¡y trÃªn CPU (cháº­m hÆ¡n).")
            device_str = 'cpu'

    logger.info("="*80)
    return device_str # Tráº£ vá» string

def main():
    config = CONFIG
    start_time = datetime.now()

    logger.info("="*80)
    logger.info("ğŸ¤– FEDERATED LEARNING Vá»šI MÃ” HÃŒNH CNN-GRU (IoT IDS)")
    logger.info("="*80)

    try:
        # BÆ°á»›c 1: Kiá»ƒm tra thiáº¿t bá»‹
        device_str = check_and_setup_gpu(config)
        config['device'] = device_str # LÆ°u string 'cuda' hoáº·c 'cpu'

        # BÆ°á»›c 2: Tá»± Ä‘á»™ng phÃ¡t hiá»‡n tham sá»‘ dá»¯ liá»‡u
        input_shape, num_classes, data_stats = auto_detect_data_parameters(
            data_dir=config['data_dir'],
            num_clients=config['num_clients']
        )
        config['input_shape'] = input_shape
        config['num_classes'] = num_classes

        # In cáº¥u hÃ¬nh cuá»‘i cÃ¹ng
        logger.info("\n" + "="*80)
        logger.info("âš™ï¸  Cáº¤U HÃŒNH CUá»I CÃ™NG")
        logger.info("="*80)
        config_str = json.dumps(config, indent=2, default=str)
        print(config_str) # DÃ¹ng print Ä‘á»ƒ hiá»ƒn thá»‹ Ä‘áº¹p
        logger.info("="*80)
        
        # --- Äá»‹nh nghÄ©a Client Function (client_fn) cho Flower ---
        # HÃ m nÃ y sáº½ Ä‘Æ°á»£c Ray gá»i Ä‘á»ƒ táº¡o client trÃªn má»™t process/GPU riÃªng
        
        def client_fn(cid: str) -> fl.client.Client:
            """Táº¡o má»™t Flower client (PyTorch)"""
            
            # 1. Táº£i dá»¯ liá»‡u cho client nÃ y
            trainloader, testloader, num_train = load_data_for_client(
                data_dir=config['data_dir'],
                client_id=int(cid),
                batch_size=config['batch_size']
            )
            
            # 2. Táº¡o model cho client nÃ y
            model = build_cnn_gru_model(
                input_shape=config['input_shape'],
                num_classes=config['num_classes']
            )
            # Chuyá»ƒn model lÃªn device (GPU/CPU)
            device = torch.device(config['device'])
            model.to(device)

            # 3. Táº¡o Flower client
            client = FlowerClient(
                cid=cid,
                model=model,
                trainloader=trainloader,
                testloader=testloader,
                device=config['device'] # Truyá»n 'cuda' hoáº·c 'cpu'
            )
            
            return client.to_client() # Chuyá»ƒn Ä‘á»•i thÃ nh Flower Client
        
        # --- Äá»‹nh nghÄ©a Strategy (Chiáº¿n lÆ°á»£c) cho Flower Server ---
        
        # HÃ m nÃ y dÃ¹ng Ä‘á»ƒ server Ä‘Ã¡nh giÃ¡ model toÃ n cá»¥c
        def get_evaluate_fn(global_test_loader, device_str):
            """
            Tráº£ vá» má»™t hÃ m Ä‘Ã¡nh giÃ¡ (evaluate_fn) cho server.
            HÃ m nÃ y sáº½ cháº¡y trÃªn server (hoáº·c 1 process riÃªng).
            """
            def evaluate(
                server_round: int,
                parameters: fl.common.NDArrays,
                config_eval: Dict[str, fl.common.Scalar],
            ) -> Optional[Tuple[float, Dict[str, fl.common.Scalar]]]:
                
                device = torch.device(device_str)
                
                # Táº¡o model táº¡m thá»i Ä‘á»ƒ Ä‘Ã¡nh giÃ¡
                model = build_cnn_gru_model(
                    input_shape=CONFIG['input_shape'],
                    num_classes=CONFIG['num_classes']
                )
                model.to(device)
                
                # Cáº­p nháº­t model vá»›i tham sá»‘ tá»« server
                params_dict = zip(model.state_dict().keys(), parameters)
                state_dict = OrderedDict({k: torch.tensor(v).to(device) for k, v in params_dict})
                model.load_state_dict(state_dict, strict=True)
                
                model.eval()
                criterion = nn.CrossEntropyLoss()
                total_loss = 0.0
                correct = 0
                total = 0

                with torch.no_grad():
                    pbar_eval = tqdm(
                        global_test_loader,
                        desc=f"[Server Eval] Round {server_round}",
                        unit="batch",
                        leave=False
                    )
                    for data, target in pbar_eval:
                        data, target = data.to(device), target.to(device)
                        output = model(data)
                        loss = criterion(output, target)
                        total_loss += loss.item() * data.size(0)
                        pred = output.argmax(dim=1)
                        correct += pred.eq(target).sum().item()
                        total += data.size(0)
                        
                        if total > 0:
                            pbar_eval.set_postfix({
                                "acc": f"{correct / total * 100:.2f}%",
                                "loss": f"{total_loss / total:.4f}"
                            })

                accuracy = correct / total if total > 0 else 0.0
                avg_loss = total_loss / total if total > 0 else 0.0
                
                logger.info(f"âœ… Round {server_round} (Server Eval): Test Acc: {accuracy*100:.2f}% | Test Loss: {avg_loss:.4f}")
                # Tráº£ vá» loss (báº¯t buá»™c), vÃ  dict metrics
                return avg_loss, {"accuracy": accuracy, "test_loss": avg_loss}
            
            return evaluate

        # --- Khá»Ÿi táº¡o Strategy ---
        logger.info("\nâ†’ Khá»Ÿi táº¡o chiáº¿n lÆ°á»£c (Strategy)...")
        
        # Táº¡o Global Test Loader (dÃ¹ng 1 láº§n cho server)
        global_test_loader = load_global_test_set(
            config['data_dir'], config['num_clients'], config['batch_size']
        )
        
        # HÃ m gá»­i config cho client (Ä‘á»ƒ client biáº¿t epochs, lr, mu)
        def fit_config(server_round: int) -> Dict:
            return {
                "server_round": server_round,
                "local_epochs": config['local_epochs'],
                "learning_rate": config['learning_rate'],
                "algorithm": config['algorithm'],
                "mu": config['mu'],
                "batch_size": config['batch_size']
            }

        if config['algorithm'] == 'fedprox':
            strategy = fl.server.strategy.FedProx(
                fraction_fit=config['client_fraction'],
                fraction_evaluate=0.0, # Táº¯t client-side evaluation (dÃ¹ng evaluate_fn thay tháº¿)
                min_fit_clients=int(config['num_clients'] * config['client_fraction']),
                min_available_clients=config['num_clients'],
                evaluate_fn=get_evaluate_fn(global_test_loader, config['device']), # ÄÃ¡nh giÃ¡ phÃ­a server
                on_fit_config_fn=fit_config, # Gá»­i config cho client
                proximal_mu=config['mu']
            )
        else: # fedavg
            strategy = fl.server.strategy.FedAvg(
                fraction_fit=config['client_fraction'],
                fraction_evaluate=0.0, # Táº¯t client-side evaluation (dÃ¹ng evaluate_fn thay tháº¿)
                min_fit_clients=int(config['num_clients'] * config['client_fraction']),
                min_available_clients=config['num_clients'],
                evaluate_fn=get_evaluate_fn(global_test_loader, config['device']),
                on_fit_config_fn=fit_config
            )
        
        logger.info(f"âœ… Strategy {config['algorithm'].upper()} Ä‘Ã£ Ä‘Æ°á»£c táº¡o.")

        # --- Cáº¥u hÃ¬nh tÃ i nguyÃªn (Cho Ray cháº¡y song song) ---
        client_resources = None
        if config['device'] == 'cuda':
            num_gpus_total = torch.cuda.device_count()
            # Chia GPU cho cÃ¡c client
            gpu_per_client = num_gpus_total / config['num_clients']
            
            # Colab free (T4) chá»‰ cÃ³ 1 GPU, Colab Pro (A100) cÃ³ 1 GPU
            # 1 CPU core cho má»—i client lÃ  Ä‘á»§
            client_resources = {"num_cpus": 1, "num_gpus": gpu_per_client}
            
            logger.info(f"\nğŸ–¥ï¸  GPU Mode: Cáº¥u hÃ¬nh Ray cho {config['num_clients']} client song song.")
            logger.info(f"   - Tá»•ng GPU: {num_gpus_total}")
            logger.info(f"   - CPU/client: {client_resources['num_cpus']}")
            logger.info(f"   - GPU/client: {client_resources['num_gpus']:.2f}")
        else:
            # Colab free chá»‰ cÃ³ 2 CPU, cháº¡y 5 client song song sáº½ ráº¥t cháº­m
            client_resources = {"num_cpus": 1}
            logger.info(f"\nğŸ’» CPU Mode: Cáº¥u hÃ¬nh Ray cho 2 client song song (tá»‘i Ä‘a).")


        # BÆ°á»›c 5: Huáº¥n luyá»‡n (DÃ¹ng Flower/Ray)
        logger.info("\n" + "="*80)
        logger.info("ğŸš€ Báº®T Äáº¦U HUáº¤N LUYá»†N FEDERATED (vá»›i Flower/Ray)")
        logger.info("="*80)
        
        # ThÃªm
        # logging.getLogger("flwr").setLevel(logging.DEBUG)

        history = fl.simulation.start_simulation(
            client_fn=client_fn,
            num_clients=config['num_clients'],
            config=fl.server.ServerConfig(num_rounds=config['num_rounds']),
            strategy=strategy,
            client_resources=client_resources
        )
        
        end_time = datetime.now()
        training_duration = (end_time - start_time).total_seconds()
        
        logger.info("\n" + "="*80)
        logger.info("ğŸ HUáº¤N LUYá»†N HOÃ€N Táº¤T!")
        logger.info("="*80)
        logger.info(f"â±ï¸  Thá»i gian huáº¥n luyá»‡n: {training_duration:.2f} giÃ¢y ({training_duration/60:.2f} phÃºt)")

        # BÆ°á»›c 6: LÆ°u káº¿t quáº£
        # Láº¥y model cuá»‘i cÃ¹ng tá»« server (tá»« strategy)
        logger.info("â†’ Äang láº¥y mÃ´ hÃ¬nh toÃ n cá»¥c cuá»‘i cÃ¹ng tá»« server...")
        server_model = build_cnn_gru_model(config['input_shape'], config['num_classes'])
        
        # Láº¥y tham sá»‘ cuá»‘i cÃ¹ng tá»« strategy
        final_params = strategy.get_parameters(config={})
        final_weights = fl.common.parameters_to_ndarrays(final_params) 
        
        params_dict = zip(server_model.state_dict().keys(), final_weights)
        state_dict = OrderedDict({k: torch.tensor(np.copy(v)) for k, v in params_dict})
        server_model.load_state_dict(state_dict, strict=True)
        logger.info("âœ… ÄÃ£ láº¥y mÃ´ hÃ¬nh thÃ nh cÃ´ng.")

        evaluate_and_save_results(
            server_model, history, config, 
            config['output_dir'], data_stats,
            training_duration, start_time, end_time
        )
        
        logger.info("\n" + "="*80)
        logger.info("ğŸ‰ ğŸ‰ ğŸ‰  HOÃ€N Táº¤T Táº¤T Cáº¢ CÃC BÆ¯á»šC  ğŸ‰ ğŸ‰ ğŸ‰")
        logger.info("="*80)

    except Exception as e:
        logger.error("\nâŒ ÄÃƒ Xáº¢Y RA Lá»–I TRONG QUÃ TRÃŒNH CHáº Y SCRIPT")
        logger.error(f"Chi tiáº¿t lá»—i: {e}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    # Thiáº¿t láº­p 'spawn' lÃ  phÆ°Æ¡ng thá»©c báº¯t Ä‘áº§u cho multiprocessing
    # Báº®T BUá»˜C pháº£i Ä‘áº·t trong block if __name__ == "__main__":
    try:
        mp.set_start_method('spawn', force=True)
        logger.info("âœ… ÄÃ£ set 'spawn' start method cho multiprocessing.")
    except RuntimeError as e:
        if "context has already been set" not in str(e):
            logger.warning(f"KhÃ´ng thá»ƒ set 'spawn' start method: {e}")
        
    main()