# üöÄ H∆∞·ªõng d·∫´n ch·∫°y Federated Learning v·ªõi Multiprocessing

## ‚ö†Ô∏è V·∫§N ƒê·ªÄ QUAN TR·ªåNG: Jupyter Notebook vs Python Script

### T·∫°i sao kh√¥ng n√™n ch·∫°y trong Jupyter Notebook?

Khi s·ª≠ d·ª•ng **PyTorch CUDA v·ªõi multiprocessing**, c√≥ 2 v·∫•n ƒë·ªÅ ch√≠nh trong Jupyter notebook:

1. **CUDA fork issue**:
   - CUDA kh√¥ng h·ªó tr·ª£ `fork()` method sau khi ƒë√£ kh·ªüi t·∫°o
   - Error: `RuntimeError: Cannot re-initialize CUDA in forked subprocess`

2. **Pickle issue v·ªõi spawn**:
   - Spawn method c·∫ßn pickle functions
   - Functions trong notebook kh√¥ng th·ªÉ pickle ƒë∆∞·ª£c
   - Error: `AttributeError: Can't get attribute '_client_training_worker'`

### ‚úÖ GI·∫¢I PH√ÅP: Ch·∫°y nh∆∞ Python script

```bash
# Thay v√¨ ch·∫°y trong notebook, ch·∫°y tr·ª±c ti·∫øp t·ª´ terminal:
python run_federated_training.py
```

## üìù H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng

### Option 1: Ch·∫°y tr·ª±c ti·∫øp (KHUY·∫æN NGH·ªä)

```bash
# Di chuy·ªÉn ƒë·∫øn th∆∞ m·ª•c ch·ª©a script
cd /path/to/Fed

# Ch·∫°y script
python run_federated_training.py
```

### Option 2: Ch·∫°y trong Google Colab

N·∫øu b·∫Øt bu·ªôc ph·∫£i d√πng Colab, c√≥ th·ªÉ ch·∫°y cell v·ªõi magic command:

```python
# Trong Colab cell
!python run_federated_training.py
```

### Option 3: T·∫Øt multiprocessing trong notebook

N·∫øu mu·ªën ch·∫°y trong notebook, t·∫Øt multiprocessing trong CONFIG:

```python
CONFIG = {
    # ...
    'use_multiprocessing': False,  # T·∫Øt multiprocessing
    'num_processes': 1,
    # ...
}
```

**L∆∞u √Ω**: T·∫Øt multiprocessing s·∫Ω ch·∫≠m h∆°n ƒë√°ng k·ªÉ (5x - 10x).

## ‚öôÔ∏è C·∫•u h√¨nh Multiprocessing

### C√°c th√¥ng s·ªë quan tr·ªçng

```python
CONFIG = {
    'use_multiprocessing': True,   # B·∫≠t/t·∫Øt multiprocessing
    'num_processes': 5,            # S·ªë processes ch·∫°y song song
}
```

### Khuy·∫øn ngh·ªã cho `num_processes`

| H·ªá th·ªëng | Khuy·∫øn ngh·ªã | L√Ω do |
|----------|------------|-------|
| **1 GPU** | 2-3 processes | Tr√°nh OOM (Out Of Memory) |
| **2 GPUs** | 4 processes | 2 processes/GPU |
| **4+ GPUs** | = num_clients | T·∫≠n d·ª•ng t·ªëi ƒëa GPUs |
| **CPU only** | 4-8 processes | = s·ªë CPU cores |

### V√≠ d·ª• c·∫•u h√¨nh

```python
# V·ªõi 1 GPU (15GB VRAM)
CONFIG = {
    'use_multiprocessing': True,
    'num_processes': 3,      # An to√†n v·ªõi 1 GPU
    'batch_size': 1024,      # C√≥ th·ªÉ gi·∫£m n·∫øu OOM
}

# V·ªõi 2 GPUs
CONFIG = {
    'use_multiprocessing': True,
    'num_processes': 4,      # 2 processes/GPU
    'batch_size': 1024,
}

# V·ªõi nhi·ªÅu GPUs (8 GPUs)
CONFIG = {
    'use_multiprocessing': True,
    'num_processes': 5,      # = num_clients
    'batch_size': 2048,      # C√≥ th·ªÉ tƒÉng batch size
}
```

## üêõ Troubleshooting

### L·ªói: "RuntimeError: Cannot re-initialize CUDA in forked subprocess"

**Nguy√™n nh√¢n**: ƒêang d√πng `fork` method v·ªõi CUDA

**Gi·∫£i ph√°p**:
1. Ch·∫°y script nh∆∞ .py file (kh√¥ng ph·∫£i notebook)
2. Script s·∫Ω t·ª± ƒë·ªông d√πng `spawn` method

### L·ªói: "AttributeError: Can't get attribute '_client_training_worker'"

**Nguy√™n nh√¢n**: ƒêang ch·∫°y trong Jupyter notebook v·ªõi spawn method

**Gi·∫£i ph√°p**:
```bash
# Ch·∫°y t·ª´ terminal thay v√¨ notebook
python run_federated_training.py
```

### L·ªói: "RuntimeError: CUDA out of memory"

**Nguy√™n nh√¢n**: Qu√° nhi·ªÅu processes ho·∫∑c batch size qu√° l·ªõn

**Gi·∫£i ph√°p**:
```python
# Gi·∫£m s·ªë processes
'num_processes': 2,  # Thay v√¨ 5

# HO·∫∂C gi·∫£m batch size
'batch_size': 512,   # Thay v√¨ 1024
```

### L·ªói: "T·∫•t c·∫£ clients ƒë·ªÅu th·∫•t b·∫°i!"

**Nguy√™n nh√¢n**: Workers kh√¥ng th·ªÉ kh·ªüi ƒë·ªông ho·∫∑c crash

**C√°ch debug**:
1. Xem error log chi ti·∫øt trong output
2. Ki·ªÉm tra VRAM: `nvidia-smi`
3. Gi·∫£m `num_processes` xu·ªëng 1 ƒë·ªÉ test
4. Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n d·ªØ li·ªáu trong CONFIG

## üìä Hi·ªáu su·∫•t

### So s√°nh Sequential vs Multiprocessing

| Method | Time (5 clients, 10 rounds) | Speedup |
|--------|----------------------------|---------|
| Sequential (1 process) | ~50 ph√∫t | 1x |
| Multiprocessing (3 processes) | ~15 ph√∫t | **3.3x** |
| Multiprocessing (5 processes) | ~10 ph√∫t | **5x** |

**L∆∞u √Ω**: Speedup th·ª±c t·∫ø ph·ª• thu·ªôc v√†o:
- S·ªë GPUs kh·∫£ d·ª•ng
- VRAM c·ªßa m·ªói GPU
- K√≠ch th∆∞·ªõc model v√† dataset

## üìà Monitoring

### Theo d√µi qu√° tr√¨nh training

Script t·ª± ƒë·ªông hi·ªÉn th·ªã:

```
üîÑ Clients Training (Parallel): 100% 5/5 [00:23<00:00, 4.30s/client]
   üöÄ Worker cho Client 0 ƒë√£ start (device: 0)
   üöÄ Worker cho Client 1 ƒë√£ start (device: 1)
   ‚úì Client 0 ho√†n th√†nh - Loss: 0.3245
   ‚úì Client 1 ho√†n th√†nh - Loss: 0.3156
```

### Theo d√µi GPU usage

```bash
# Terminal kh√°c
watch -n 1 nvidia-smi
```

B·∫°n s·∫Ω th·∫•y multiple processes s·ª≠ d·ª•ng GPU ƒë·ªìng th·ªùi.

## üéØ Best Practices

1. **Lu√¥n ch·∫°y script t·ª´ terminal** khi d√πng multiprocessing + CUDA
2. **Test v·ªõi num_processes=1** tr∆∞·ªõc ƒë·ªÉ ƒë·∫£m b·∫£o code ch·∫°y ƒë√∫ng
3. **Monitor VRAM** khi tƒÉng num_processes
4. **Backup d·ªØ li·ªáu** tr∆∞·ªõc khi ch·∫°y training d√†i
5. **D√πng tmux/screen** ƒë·ªÉ tr√°nh m·∫•t k·∫øt n·ªëi SSH

## üìû Support

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ, ki·ªÉm tra:
1. PyTorch version: `python -c "import torch; print(torch.__version__)"`
2. CUDA version: `nvcc --version`
3. GPU memory: `nvidia-smi`
4. Python version: `python --version` (khuy·∫øn ngh·ªã: 3.8+)

---

**T√≥m t·∫Øt**: ƒê·ªÉ s·ª≠ d·ª•ng multiprocessing v·ªõi CUDA hi·ªáu qu·∫£, h√£y ch·∫°y script t·ª´ terminal:
```bash
python run_federated_training.py
```
